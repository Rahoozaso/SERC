{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b4486",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import logging\n",
    "import json # 상세 로그(dict) 출력을 위해\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- 프로젝트 루트 경로 설정 ---\n",
    "try:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"Added project root to path: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- src 모듈 임포트 ---\n",
    "try:\n",
    "    from src.utils import load_config, load_jsonl\n",
    "    # 평가 함수 임포트 (사례 탐색용)\n",
    "    from src.evaluation import calculate_qa_metrics_for_item # PreciseWikiQA용\n",
    "    # from src.evaluation import evaluate_factscore_official # FactScore용 (구현 필요)\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing src modules: {e}\")\n",
    "\n",
    "# 로깅 및 스타일 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "pd.set_option('display.max_colwidth', 500) # DataFrame에서 긴 텍스트 잘리지 않게"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc384c20",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. 모든 결과 파일 로드 ---\n",
    "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"configs\", \"config.yaml\")\n",
    "config = load_config(CONFIG_PATH)\n",
    "RESULTS_BASE_DIR = os.path.join(PROJECT_ROOT, config.get('results_base_dir', 'results'))\n",
    "print(f\"Loading all results from: {RESULTS_BASE_DIR}\")\n",
    "\n",
    "all_results_long = []\n",
    "# `results/` 하위의 모든 .jsonl 파일 탐색 (hyperparam_tuning 제외)\n",
    "file_paths = glob.glob(os.path.join(RESULTS_BASE_DIR, \"**\", \"*.jsonl\"), recursive=True)\n",
    "file_paths = [p for p in file_paths if 'hyperparam_tuning' not in p.replace(\"\\\\\", \"/\")]\n",
    "\n",
    "if not file_paths:\n",
    "    logger.error(f\"[오류] {RESULTS_BASE_DIR}에서 분석할 결과 파일을 찾을 수 없습니다.\")\n",
    "else:\n",
    "    print(f\"Found {len(file_paths)} result files to analyze.\")\n",
    "\n",
    "# --- 2. 파일명 파싱 및 데이터 통합 ---\n",
    "# (3_main_results_analysis의 로직과 유사)\n",
    "import re\n",
    "serc_regex = re.compile(r\"serc_t(\\d+)_mf(\\d+)\")\n",
    "dense_regex = re.compile(r\"serc_dense_iterative_t(\\d+)\") # dense-iterative\n",
    "dense_1pass_regex = re.compile(r\"serc_dense_1pass\") # dense-1pass\n",
    "\n",
    "for f_path in file_paths:\n",
    "    try:\n",
    "        filename = os.path.basename(f_path)\n",
    "        parts = f_path.replace(\"\\\\\", \"/\").split('/')\n",
    "        dataset_name = parts[-2]\n",
    "        model_name = parts[-3].replace('_', '/')\n",
    "        \n",
    "        method = \"unknown\"\n",
    "        t_max = None\n",
    "        max_facts = None\n",
    "        \n",
    "        if filename.startswith(\"baseline\"):\n",
    "            method = \"Baseline\"\n",
    "        elif filename.startswith(\"cove\"):\n",
    "            method = \"CoVe\"\n",
    "        elif dense_1pass_regex.search(filename):\n",
    "             method = \"SERC (Dense, 1-pass)\"\n",
    "             t_max = 1\n",
    "        elif dense_regex.search(filename):\n",
    "            method = \"SERC (Dense, Iter)\"\n",
    "            match_dense = dense_regex.search(filename)\n",
    "            if match_dense: t_max = int(match_dense.group(1))\n",
    "        elif filename.startswith(\"serc\"):\n",
    "            match_serc = serc_regex.search(filename)\n",
    "            if match_serc:\n",
    "                t_max = int(match_serc.group(1))\n",
    "                max_facts = int(match_serc.group(2))\n",
    "                method = \"SERC (1-pass)\" if t_max == 1 else \"SERC (Iterative)\"\n",
    "            else:\n",
    "                method = \"SERC (Unknown)\"\n",
    "        else:\n",
    "             continue # 모르는 파일\n",
    "\n",
    "        # 결과 파일 로드\n",
    "        results_data = load_jsonl(f_path)\n",
    "        \n",
    "        for item in results_data:\n",
    "            # 평가에 필요한 키 추출\n",
    "            query = item.get('query', item.get('question'))\n",
    "            # data_loader가 저장한 정답 (e.g., ['John F. Kennedy'] or ['No...'])\n",
    "            ground_truth_list = item.get('answers', item.get('correct_answers_truthfulqa'))\n",
    "            # 모델의 최종 답변\n",
    "            final_output = item.get(\"method_result\", {}).get(\"final_output\")\n",
    "            # SERC의 상세 로그 (분석용)\n",
    "            serc_history_log = item.get(\"method_result\", {}).get(\"serc_result\", {})\n",
    "            \n",
    "            all_results_long.append({\n",
    "                \"model\": model_name,\n",
    "                \"dataset\": dataset_name,\n",
    "                \"method\": method,\n",
    "                \"t_max\": t_max,\n",
    "                \"max_facts\": max_facts,\n",
    "                \"query\": query,\n",
    "                \"ground_truth\": ground_truth_list, # 정답 리스트\n",
    "                \"final_output\": final_output, # 모델 예측\n",
    "                \"serc_history_log\": serc_history_log # SERC 상세 로그\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {f_path}: {e}\", exc_info=True)\n",
    "\n",
    "# --- 3. 최종 통합 데이터프레임 생성 ---\n",
    "df_all = pd.DataFrame(all_results_long)\n",
    "\n",
    "if df_all.empty:\n",
    "    logger.error(\"처리할 결과 데이터가 없습니다. 스크립트를 종료합니다.\")\n",
    "else:\n",
    "    print(\"\\n--- 모든 결과 데이터 통합 완료 ---\")\n",
    "    print(f\"Total entries loaded: {len(df_all)}\")\n",
    "    print(\"DataFrame Info:\")\n",
    "    df_all.info()\n",
    "    print(\"\\nAvailable Methods:\")\n",
    "    print(df_all['method'].value_counts())\n",
    "    print(\"\\nAvailable Datasets:\")\n",
    "    print(df_all['dataset'].value_counts())\n",
    "    display(df_all.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb991bc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 사례 탐색(Case Finding)을 위한 평가 함수 ---\n",
    "\n",
    "def evaluate_single_item(prediction, ground_truths, dataset_name):\n",
    "    \"\"\"단일 항목의 정답 여부를 True/False로 반환합니다 (간단 버전)\"\"\"\n",
    "    if prediction is None or ground_truths is None:\n",
    "        return False\n",
    "        \n",
    "    if 'precisewikiqa' in dataset_name.lower():\n",
    "        # EM/F1 기반 평가\n",
    "        metrics = calculate_qa_metrics_for_item(prediction, ground_truths)\n",
    "        return metrics['em'] > 0.99 # EM 1.0\n",
    "    \n",
    "    elif 'truthfulqa' in dataset_name.lower():\n",
    "        # TruthfulQA 평가는 복잡함.\n",
    "        # 여기서는 임시로 'correct_answers'에 예측이 포함되는지 확인\n",
    "        # TODO: 공식 TruthfulQA 평가 로직 연동 필요\n",
    "        norm_pred = prediction.strip().lower()\n",
    "        norm_correct = str(ground_truths).lower() # ground_truths가 문자열일 수 있음\n",
    "        return norm_pred in norm_correct and norm_pred not in str(item.get('incorrect_answers_truthfulqa', '')).lower()\n",
    "        \n",
    "    elif 'longwiki' in dataset_name.lower():\n",
    "        # FactScore는 항목별 True/False 반환이 어려움\n",
    "        # 정성 분석에서는 수동 비교가 필요할 수 있음\n",
    "        # 여기서는 임시로 '오류 없음'을 가정 (수동 분석 필요)\n",
    "        return pd.NA # N/A (Not Applicable)\n",
    "    \n",
    "    return False\n",
    "\n",
    "if not df_all.empty:\n",
    "    # --- 4. 분석/비교를 위해 데이터프레임 피봇(Pivot) ---\n",
    "    # (쿼리/모델/데이터셋별로 각 방법론의 결과를 열로 펼침)\n",
    "    \n",
    "    # 평가 수행 (True/False)\n",
    "    # (LongWiki는 FactScore 필요하므로 여기서는 QA 벤치마크만 예시로 수행)\n",
    "    qa_datasets = [d for d in df_all['dataset'].unique() if 'precisewikiqa' in d.lower() or 'truthfulqa' in d.lower()]\n",
    "    if qa_datasets:\n",
    "        df_qa = df_all[df_all['dataset'].isin(qa_datasets)].copy()\n",
    "        df_qa['is_correct'] = df_qa.apply(\n",
    "            lambda row: evaluate_single_item(row['final_output'], row['ground_truth'], row['dataset']),\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        df_qa = pd.DataFrame(columns=['query', 'model', 'dataset', 'method', 'is_correct'])\n",
    "        logger.warning(\"QA 데이터셋(PreciseWikiQA, TruthfulQA) 결과가 없어 사례 탐색이 제한됩니다.\")\n",
    "\n",
    "\n",
    "    # LongWiki 데이터 (FactScore는 3_main_results... 에서 계산된 점수 사용 가정)\n",
    "    df_longwiki = df_all[df_all['dataset'] == 'hallulens_longwiki'].copy()\n",
    "\n",
    "    print(\"\\n--- QA 데이터(PreciseWikiQA, TruthfulQA) 평가 완료 (간단 버전) ---\")\n",
    "    display(df_qa[['model', 'dataset', 'method', 'is_correct']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7151eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 5. 상세 로그 출력을 위한 헬퍼 함수 ---\n",
    "\n",
    "def display_serc_log(serc_history_log: Dict[str, Any]):\n",
    "    \"\"\"SERC 실행 기록(history dict)을 단계별로 예쁘게 출력합니다.\"\"\"\n",
    "    \n",
    "    if not serc_history_log:\n",
    "        print(\"[[ SERC 로그가 없거나 비어있습니다. ]]\")\n",
    "        return\n",
    "        \n",
    "    print(f\"--- [Query] ---\\n{serc_history_log.get('query')}\\n\")\n",
    "    print(f\"--- [Initial Baseline (t=0)] ---\\n{serc_history_log.get('initial_baseline')}\\n\")\n",
    "    \n",
    "    cycles = serc_history_log.get('cycles', [])\n",
    "    if not cycles:\n",
    "        print(\"[[ 실행된 사이클이 없습니다. (e.g., 사실 추출 실패) ]]\")\n",
    "        \n",
    "    for cycle in cycles:\n",
    "        t = cycle.get('cycle')\n",
    "        print(f\"\\n{'='*20} [ Cycle {t} ] {'='*20}\")\n",
    "        \n",
    "        # 2. 사실 추출\n",
    "        facts = cycle.get('steps', {}).get('2_fact_extraction', {}).get('parsed_facts', {})\n",
    "        print(f\"\\n[2. Fact Extraction] (총 {len(facts)}개)\")\n",
    "        for fi, f_text in facts.items():\n",
    "            print(f\"  - {fi}: {f_text}\")\n",
    "            \n",
    "        # 3. 신드롬 생성\n",
    "        syndrome_step = cycle.get('steps', {}).get('3_syndrome_generation', {})\n",
    "        if not syndrome_step:\n",
    "             # (Dense 모드일 경우)\n",
    "             syndrome_step = cycle.get('steps', {}).get('3_syndrome_generation_dense', {})\n",
    "             print(\"\\n[3. Syndrome Generation (Dense Mode)]\")\n",
    "        else:\n",
    "             print(\"\\n[3. Syndrome Generation (Low-Density Mode)]\")\n",
    "        \n",
    "        # 3a. 태깅\n",
    "        tags = syndrome_step.get('3a_tags', {})\n",
    "        if tags:\n",
    "            print(\"  [3a. Tags]:\")\n",
    "            print(\"    \" + \", \".join([f\"{fi}:'{tag}'\" for fi, tag in tags.items()]))\n",
    "            \n",
    "        # 3b. 그룹화\n",
    "        groups = syndrome_step.get('3b2_chunked_groups', {})\n",
    "        if groups:\n",
    "            print(\"  [3b. Groups]:\")\n",
    "            for tag, f_ids in groups.items():\n",
    "                print(f\"    - {tag}: {f_ids}\")\n",
    "                \n",
    "        # 3c/d. 검증 및 신드롬\n",
    "        validations = syndrome_step.get('3c_3d_details', [])\n",
    "        syndrome_found = {}\n",
    "        if validations:\n",
    "            print(\"  [3c/d. Validation & Syndrome]:\")\n",
    "            for detail in validations:\n",
    "                # (Dense 모드 로그)\n",
    "                if 'question' in detail and 'verified_answer' in detail:\n",
    "                    print(f\"    - Fact: {detail.get('fact_id')}\")\n",
    "                    print(f\"      Q: {detail.get('question')}\")\n",
    "                    print(f\"      A: {detail.get('verified_answer')}\")\n",
    "                    print(f\"      Result: {detail.get('result')}\")\n",
    "                    if detail.get('result') == \"[예]\":\n",
    "                        syndrome_found[detail.get('fact_id')] = detail.get('fact_text')\n",
    "                # (Low-Density 모드 로그 - 그룹별)\n",
    "                elif 'tag' in detail:\n",
    "                    print(f\"    - Group: '{detail.get('tag')}'\")\n",
    "                    print(f\"      Q: {detail.get('question')}\")\n",
    "                    print(f\"      A: {detail.get('verified_answer')}\")\n",
    "                    for val_item in detail.get('validations', []):\n",
    "                        print(f\"        - {val_item.get('fact_id')}: {val_item.get('fact_text')[:50]}... -> {val_item.get('result')}\")\n",
    "                        if val_item.get('result') == \"[예]\":\n",
    "                            syndrome_found[val_item.get('fact_id')] = val_item.get('fact_text')\n",
    "        \n",
    "        if not syndrome_found:\n",
    "             print(\"\\n  [4c. Convergence Check]: 신드롬 없음 (No Syndrome).\")\n",
    "        \n",
    "        # 5. 교정\n",
    "        corrections = cycle.get('steps', {}).get('5_correction', [])\n",
    "        if corrections:\n",
    "            print(\"\\n[5. Correction]:\")\n",
    "            for corr_item in corrections:\n",
    "                print(f\"  - Targeting Fact: {corr_item.get('fact_id')} ('{corr_item.get('original_fact', '')[:50]}...')\")\n",
    "                print(f\"    Status: {corr_item.get('status')}\")\n",
    "                print(f\"    Found Sentence: {corr_item.get('found_sentence')}\")\n",
    "                print(f\"    Corrected Fact: {corr_item.get('corrected_fact')}\")\n",
    "                print(f\"    Rewritten Sentence: {corr_item.get('rewritten_sentence')}\")\n",
    "        \n",
    "        print(f\"\\n--- [Baseline After Cycle {t}] ---\\n{cycle.get('baseline_after_cycle')}\\n\")\n",
    "\n",
    "    print(f\"\\n{'='*20} [ Final Result ] {'='*20}\")\n",
    "    print(f\"--- [Final Baseline] ---\\n{serc_history_log.get('final_baseline')}\")\n",
    "    print(f\"Termination: {serc_history_log.get('termination_reason')}\")\n",
    "\n",
    "\n",
    "# --- 6. 사례 탐색기 (Case Finder) ---\n",
    "def find_cases(df_analysis, model, dataset, case_type):\n",
    "    \"\"\"연구 계획서 4.6.2 기준에 따라 사례 탐색\"\"\"\n",
    "    \n",
    "    # 모델/데이터셋 필터링\n",
    "    df_filtered = df_analysis[\n",
    "        (df_analysis['model'] == model) &\n",
    "        (df_analysis['dataset'] == dataset)\n",
    "    ].copy()\n",
    "    \n",
    "    if df_filtered.empty:\n",
    "        print(f\"'{model}' / '{dataset}'에 대한 결과를 찾을 수 없습니다.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # (쿼리별로 방법론 피봇)\n",
    "    df_pivot = df_filtered.pivot_table(\n",
    "        index='query', # 쿼리 기준\n",
    "        columns='method', # 방법론을 열로\n",
    "        values=['is_correct', 'final_output', 'serc_history_log'], # 필요한 값\n",
    "        aggfunc='first' # 중복 제거 (보통 쿼리당 하나)\n",
    "    ).reset_index()\n",
    "    \n",
    "    # 컬럼 이름 재구성 (e.g., ('is_correct', 'Baseline'))\n",
    "    df_pivot.columns = ['_'.join(col).strip('_') if isinstance(col, tuple) else col for col in df_pivot.columns.values]\n",
    "    \n",
    "    print(f\"--- {model} / {dataset} / {case_type} 사례 탐색 ---\")\n",
    "\n",
    "    try:\n",
    "        if case_type == \"RQ3_ITERATIVE_SUCCESS\" and 'is_correct_SERC (1-pass)' in df_pivot.columns:\n",
    "            # (Baseline=False) AND (1-pass=False) AND (Iterative=True)\n",
    "            cases = df_pivot[\n",
    "                (df_pivot['is_correct_Baseline'] == False) &\n",
    "                (df_pivot['is_correct_SERC (1-pass)'] == False) &\n",
    "                (df_pivot['is_correct_SERC (Iterative)'] == True)\n",
    "            ]\n",
    "            \n",
    "        elif case_type == \"FAILURE_ANALYSIS\" and 'is_correct_SERC (Iterative)' in df_pivot.columns:\n",
    "            # (Iterative=False)\n",
    "            cases = df_pivot[\n",
    "                (df_pivot['is_correct_SERC (Iterative)'] == False)\n",
    "            ]\n",
    "            \n",
    "        elif case_type == \"ERROR_AMPLIFICATION\" and 'is_correct_Baseline' in df_pivot.columns:\n",
    "            # (Baseline=True) AND (Iterative=False)\n",
    "             cases = df_pivot[\n",
    "                (df_pivot['is_correct_Baseline'] == True) &\n",
    "                (df_pivot['is_correct_SERC (Iterative)'] == False)\n",
    "            ]\n",
    "        else:\n",
    "            print(f\"'{case_type}' 사례를 찾을 수 없거나, 필요한 방법론(e.g., SERC (1-pass))의 결과가 없습니다.\")\n",
    "            return pd.DataFrame()\n",
    "            \n",
    "        print(f\"Found {len(cases)} cases.\")\n",
    "        return cases\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"분석에 필요한 컬럼이 부족합니다: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477da721",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 7. 분석 실행 ---\n",
    "# (분석하려는 모델과 데이터셋 지정)\n",
    "TARGET_MODEL = \"meta-llama/Llama-3.1-8B-Instruct\" # 예시 모델\n",
    "TARGET_DATASET = \"hallulens_precisewikiqa\" # 예시 QA 데이터셋\n",
    "\n",
    "# (df_qa는 셀 3에서 QA 데이터셋만 필터링하여 'is_correct'를 계산한 DataFrame임)\n",
    "if 'df_qa' in locals() and not df_qa.empty:\n",
    "    \n",
    "    # --- 사례 1: RQ3 (반복 교정) 증명 사례 ---\n",
    "    # (이 사례는 T_max=1 (1-pass)와 T_max=2+ (Iterative) 결과를 모두 실행해야 찾을 수 있음)\n",
    "    rq3_cases = find_cases(df_qa, TARGET_MODEL, TARGET_DATASET, \"RQ3_ITERATIVE_SUCCESS\")\n",
    "    display(rq3_cases.head())\n",
    "\n",
    "    # --- 사례 2: 교정 실패 사례 ---\n",
    "    failure_cases = find_cases(df_qa, TARGET_MODEL, TARGET_DATASET, \"FAILURE_ANALYSIS\")\n",
    "    if not failure_cases.empty:\n",
    "        print(\"\\n--- 교정 실패 사례 (상세 로그 1개) ---\")\n",
    "        \n",
    "        # 첫 번째 실패 사례의 상세 로그 가져오기\n",
    "        # (df_all에서 원본 로그를 찾아야 함)\n",
    "        failed_query = failure_cases.iloc[0]['query']\n",
    "        \n",
    "        failed_log_entry = df_all[\n",
    "            (df_all['query'] == failed_query) &\n",
    "            (df_all['model'] == TARGET_MODEL) &\n",
    "            (df_all['method'] == 'SERC (Iterative)') # 실패한 SERC Iterative 로그\n",
    "        ]\n",
    "        \n",
    "        if not failed_log_entry.empty:\n",
    "            failed_log_dict = failed_log_entry.iloc[0]['serc_history_log']\n",
    "            display_serc_log(failed_log_dict) # 상세 로그 출력\n",
    "        else:\n",
    "            print(\"실패 사례에 대한 상세 로그를 찾을 수 없습니다.\")\n",
    "            \n",
    "    # --- 사례 3: 오류 증폭 사례 ---\n",
    "    amp_cases = find_cases(df_qa, TARGET_MODEL, TARGET_DATASET, \"ERROR_AMPLIFICATION\")\n",
    "    display(amp_cases.head())\n",
    "    \n",
    "    # (LongWiki (df_longwiki)에 대한 분석은 FactScore 점수 기반으로 유사하게 수행)\n",
    "    \n",
    "else:\n",
    "    print(\"\\n분석할 QA 데이터가 없습니다. 셀 2와 3을 먼저 실행하세요.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
