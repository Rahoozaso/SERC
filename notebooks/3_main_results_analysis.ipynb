{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccfd53b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import logging\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "# --- 프로젝트 루트 경로 설정 ---\n",
    "try:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n",
    "except NameError:\n",
    "    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(os.getcwd())))\n",
    "\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "    print(f\"Added project root to path: {PROJECT_ROOT}\")\n",
    "\n",
    "# --- src 모듈 임포트 ---\n",
    "try:\n",
    "    from src.utils import load_config, load_jsonl\n",
    "    # 평가 함수 임포트\n",
    "    from src.evaluation import calculate_qa_metrics_for_item # PreciseWikiQA용\n",
    "    # from src.evaluation import evaluate_factscore_official # FactScore용 (구현 필요)\n",
    "    # from src.evaluation import evaluate_truthfulqa_official # TruthfulQA용 (구현 필요)\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing src modules: {e}\")\n",
    "\n",
    "# 로깅 및 스타일 설정\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(module)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "sns.set_theme(style=\"whitegrid\", context=\"talk\")\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries and modules imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c22835",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. 설정 로드 ---\n",
    "CONFIG_PATH = os.path.join(PROJECT_ROOT, \"configs\", \"config.yaml\")\n",
    "config = load_config(CONFIG_PATH)\n",
    "RESULTS_BASE_DIR = os.path.join(PROJECT_ROOT, config.get('results_base_dir', 'results'))\n",
    "print(f\"Loading all results from: {RESULTS_BASE_DIR}\")\n",
    "\n",
    "# --- 2. 평가 함수 매핑 (플레이스홀더 포함) ---\n",
    "def evaluate_factscore_placeholder(results_data, config):\n",
    "    \"\"\"FactScore 공식 스크립트 연동 전 임시 함수\"\"\"\n",
    "    logger.warning(\"Using PLACEHOLDER for FactScore evaluation!\")\n",
    "    # TODO: 'final_output'과 'query'/'topic'을 FactScore 공식 스크립트에 전달\n",
    "    # 예시: return factscore.get_score(...)\n",
    "    # 임시로 랜덤 점수 반환 (분석용)\n",
    "    import random\n",
    "    return {'factscore': random.uniform(0.5, 0.8)} \n",
    "\n",
    "def evaluate_truthfulqa_placeholder(results_data, config):\n",
    "    \"\"\"TruthfulQA 공식 스크립트 연동 전 임시 함수\"\"\"\n",
    "    logger.warning(\"Using PLACEHOLDER for TruthfulQA evaluation!\")\n",
    "    # TODO: 'final_output', 'correct_answers_truthfulqa' 등을 공식 스크립트에 전달\n",
    "    # 예시: return truthfulqa.evaluate(...)\n",
    "    # 임시로 랜덤 점수 반환 (분석용)\n",
    "    import random\n",
    "    return {'accuracy': random.uniform(0.4, 0.7)}\n",
    "\n",
    "def evaluate_qa_benchmark(results_data: List[Dict[str, Any]], config: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"PreciseWikiQA / MultiSpanQA 평가 (src/evaluation.py 재사용)\"\"\"\n",
    "    logger.info(\"Running QA (EM/F1) evaluation...\")\n",
    "    em_sum = 0\n",
    "    f1_sum = 0\n",
    "    evaluated_count = 0\n",
    "    for item in results_data:\n",
    "        pred = item.get(\"method_result\", {}).get(\"final_output\")\n",
    "        gold_list = item.get(\"answers\") # data_loader가 'answers' 키로 저장\n",
    "        if pred is not None and gold_list is not None:\n",
    "            metrics = calculate_qa_metrics_for_item(pred, gold_list)\n",
    "            em_sum += metrics['em']\n",
    "            f1_sum += metrics['f1']\n",
    "            evaluated_count += 1\n",
    "    em_avg = (em_sum / evaluated_count) * 100 if evaluated_count > 0 else 0\n",
    "    f1_avg = (f1_sum / evaluated_count) * 100 if evaluated_count > 0 else 0\n",
    "    return {\"accuracy_em\": em_avg, \"f1_score\": f1_avg, \"count\": evaluated_count}\n",
    "\n",
    "# 데이터셋 이름(키)과 실제 평가 함수 매핑\n",
    "EVALUATION_MAP = {\n",
    "    'hallulens_longwiki': evaluate_factscore_placeholder, # TODO: 공식 FactScore 함수로 교체\n",
    "    'hallulens_precisewikiqa': evaluate_qa_benchmark,\n",
    "    'truthfulqa': evaluate_truthfulqa_placeholder, # TODO: 공식 TruthfulQA 함수로 교체\n",
    "    # 'multispanqa_dev': evaluate_qa_benchmark, # MultiSpanQA 사용 시\n",
    "}\n",
    "\n",
    "# --- 3. 모든 결과 파일 로드, 평가, 집계 ---\n",
    "all_results_data = []\n",
    "# `results/` 하위의 모든 .jsonl 파일 탐색\n",
    "file_paths = glob.glob(os.path.join(RESULTS_BASE_DIR, \"**\", \"*.jsonl\"), recursive=True)\n",
    "\n",
    "if not file_paths:\n",
    "    logger.error(f\"[오류] {RESULTS_BASE_DIR}에서 결과 파일을 찾을 수 없습니다. 실험을 먼저 실행하세요.\")\n",
    "else:\n",
    "    print(f\"Found {len(file_paths)} result files to analyze.\")\n",
    "\n",
    "# 파일명에서 정보 파싱을 위한 정규 표현식 (예시)\n",
    "# serc_t2_mf5_... -> method=serc, t_max=2, max_facts=5\n",
    "# serc_dense_iterative_t2... -> method=serc_dense, t_max=2\n",
    "# baseline... -> method=baseline\n",
    "# cove... -> method=cove\n",
    "# serc_t1_mf5... -> method=serc_1pass (t=1일 때)\n",
    "serc_regex = re.compile(r\"serc_t(\\d+)_mf(\\d+)\")\n",
    "dense_regex = re.compile(r\"serc_dense_iterative_t(\\d+)\")\n",
    "\n",
    "for f_path in file_paths:\n",
    "    try:\n",
    "        filename = os.path.basename(f_path)\n",
    "        parts = f_path.replace(\"\\\\\", \"/\").split('/')\n",
    "        dataset_name = parts[-2]\n",
    "        model_name = parts[-3].replace('_', '/') # Qwen_Qwen... -> Qwen/Qwen...\n",
    "        \n",
    "        # 파일명 기반으로 방법론(method) 및 파라미터 파싱\n",
    "        method = \"unknown\"\n",
    "        t_max = None\n",
    "        max_facts = None\n",
    "        \n",
    "        if filename.startswith(\"baseline\"):\n",
    "            method = \"Baseline\"\n",
    "        elif filename.startswith(\"cove\"):\n",
    "            method = \"CoVe\"\n",
    "        elif \"dense\" in filename:\n",
    "            method = \"SERC (Dense)\"\n",
    "            match_dense = dense_regex.search(filename)\n",
    "            if match_dense: t_max = int(match_dense.group(1))\n",
    "        elif filename.startswith(\"serc\"):\n",
    "            match_serc = serc_regex.search(filename)\n",
    "            if match_serc:\n",
    "                t_max = int(match_serc.group(1))\n",
    "                max_facts = int(match_serc.group(2))\n",
    "                method = \"SERC (1-pass)\" if t_max == 1 else \"SERC (Iterative)\"\n",
    "            else:\n",
    "                method = \"SERC (Unknown)\"\n",
    "        \n",
    "        if method == \"unknown\":\n",
    "            logger.warning(f\"Skipping file (method unknown): {filename}\")\n",
    "            continue\n",
    "            \n",
    "        if dataset_name not in EVALUATION_MAP:\n",
    "            logger.warning(f\"Skipping file (no evaluator for dataset '{dataset_name}'): {filename}\")\n",
    "            continue\n",
    "\n",
    "        # --- 4. 실제 평가 수행 ---\n",
    "        logger.info(f\"Evaluating: {model_name} / {dataset_name} / {method} (T={t_max}, MF={max_facts})\")\n",
    "        results_data = load_jsonl(f_path)\n",
    "        if not results_data:\n",
    "            logger.warning(f\"Result file is empty: {f_path}\")\n",
    "            continue\n",
    "            \n",
    "        # 데이터셋에 맞는 평가 함수 호출\n",
    "        eval_func = EVALUATION_MAP[dataset_name]\n",
    "        metrics = eval_func(results_data, config)\n",
    "        \n",
    "        # 토큰 사용량 집계 (예시: SERC 결과에서 누적 토큰 가져오기)\n",
    "        # TODO: SERC 함수가 'total_tokens_used'를 반환하도록 수정 필요\n",
    "        total_tokens = 0\n",
    "        if method.startswith(\"SERC\"):\n",
    "            # 예시: 각 항목의 serc_result에서 'total_tokens_used' 키 값 합산\n",
    "            try:\n",
    "                total_tokens = sum(item['method_result']['serc_result'].get('total_tokens_used', 0) for item in results_data)\n",
    "            except KeyError:\n",
    "                total_tokens = 0 # 구현 안된 경우\n",
    "        \n",
    "        # 집계용 딕셔너리 생성\n",
    "        entry = {\n",
    "            \"Model\": model_name,\n",
    "            \"Dataset\": dataset_name,\n",
    "            \"Method\": method,\n",
    "            \"T_max\": t_max,\n",
    "            \"Max_Facts\": max_facts,\n",
    "            \"Total_Tokens\": total_tokens, # 총 토큰\n",
    "            \"Avg_Tokens\": total_tokens / len(results_data) if results_data else 0,\n",
    "            **metrics # 평가 함수가 반환한 모든 지표 (factscore, f1_score 등)\n",
    "        }\n",
    "        all_results_data.append(entry)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {f_path}: {e}\", exc_info=True)\n",
    "\n",
    "# --- 5. 최종 집계 데이터프레임 생성 ---\n",
    "df_final_results = pd.DataFrame(all_results_data)\n",
    "print(\"\\n--- 최종 집계 결과 (Head) ---\")\n",
    "display(df_final_results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2e0bfb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RQ1 (SOTA) & RQ4 (Scaling)\n",
    "# - 비교 대상: Baseline, CoVe, SERC (Iterative)\n",
    "# - (참고) T_max, Max_Facts 값이 여러 개일 수 있으므로,\n",
    "#   예비 실험(2_pilot...)에서 결정된 최적의 값 (e.g., T=2, MF=5)으로 필터링 필요\n",
    "\n",
    "OPTIMAL_T_MAX = 2 # 예비 실험에서 결정된 값 (예시)\n",
    "OPTIMAL_MAX_FACTS = 5 # 예비 실험에서 결정된 값 (예시)\n",
    "\n",
    "# 비교 대상 필터링\n",
    "methods_to_compare_rq1 = [\"Baseline\", \"CoVe\", \"SERC (Iterative)\"]\n",
    "df_rq1_rq4 = df_final_results[\n",
    "    (df_final_results['Method'].isin(methods_to_compare_rq1))\n",
    "].copy()\n",
    "\n",
    "# SERC (Iterative)는 최적 파라미터 사용 결과만 선택\n",
    "df_rq1_rq4 = df_rq1_rq4[\n",
    "    (df_rq1_rq4['Method'] != 'SERC (Iterative)') | \n",
    "    ((df_rq1_rq4['Method'] == 'SERC (Iterative)') & \n",
    "     (df_rq1_rq4['T_max'] == OPTIMAL_T_MAX) & \n",
    "     (df_rq1_rq4['Max_Facts'] == OPTIMAL_MAX_FACTS))\n",
    "]\n",
    "\n",
    "# (필요시 'Method' 이름 정리)\n",
    "# df_rq1_rq4['Method'] = df_rq1_rq4['Method'].replace({'SERC (Iterative)': 'SERC (Ours)'})\n",
    "\n",
    "print(f\"\\n--- RQ1 & RQ4 분석용 테이블 (최적 파라미터 T={OPTIMAL_T_MAX}, MF={OPTIMAL_MAX_FACTS} 기준) ---\")\n",
    "# 모델, 데이터셋, 방법론별 성능 지표 (예: f1_score)\n",
    "metrics_to_show = ['Model', 'Dataset', 'Method', 'f1_score', 'accuracy_em', 'factscore', 'Avg_Tokens']\n",
    "# df_rq1_rq4_pivot = df_rq1_rq4.pivot_table(\n",
    "#     index=['Model', 'Dataset'],\n",
    "#     columns='Method',\n",
    "#     values=['f1_score', 'factscore', 'accuracy_em'] # 보여줄 지표\n",
    "# )\n",
    "# display(df_rq1_rq4_pivot)\n",
    "display(df_rq1_rq4[[col for col in metrics_to_show if col in df_rq1_rq4.columns]].sort_values(by=['Model', 'Dataset']))\n",
    "\n",
    "\n",
    "# --- RQ4 시각화 (예시: 8B 모델 대비 30B 모델 성능) ---\n",
    "# (데이터셋별로 그래프 그리기)\n",
    "for dataset in df_rq1_rq4['Dataset'].unique():\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(\n",
    "        data=df_rq1_rq4[df_rq1_rq4['Dataset'] == dataset],\n",
    "        x='Model',\n",
    "        y='f1_score', # 또는 'factscore'\n",
    "        hue='Method'\n",
    "    )\n",
    "    plt.title(f\"RQ1 & RQ4: {dataset} 성능 비교\")\n",
    "    plt.ylabel(\"Performance (F1 Score or FactScore)\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "    plt.xticks(rotation=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42462c94",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RQ2 (Low-Density vs Dense)\n",
    "# - 비교 대상: SERC (Iterative) vs SERC (Dense)\n",
    "# - 데이터셋: HalluLens (LongWiki)\n",
    "# - T_max는 동일한 값 (e.g., 최적 T_max)으로 필터링\n",
    "\n",
    "OPTIMAL_T_MAX = 2 # 예비 실험에서 결정된 값\n",
    "\n",
    "methods_to_compare_rq2 = [\"SERC (Iterative)\", \"SERC (Dense)\"]\n",
    "df_rq2 = df_final_results[\n",
    "    (df_final_results['Method'].isin(methods_to_compare_rq2)) &\n",
    "    (df_final_results['Dataset'] == 'hallulens_longwiki') & # LongWiki 벤치마크\n",
    "    (df_final_results['T_max'] == OPTIMAL_T_MAX) # 동일 T_max\n",
    "].copy()\n",
    "\n",
    "# SERC (Iterative)는 최적 Max_Facts 사용\n",
    "df_rq2 = df_rq2[\n",
    "    (df_rq2['Method'] != 'SERC (Iterative)') |\n",
    "    (df_rq2['Max_Facts'] == OPTIMAL_MAX_FACTS)\n",
    "]\n",
    "# (SERC (Dense)는 max_facts가 없을 수 있으므로 N/A 처리 필요)\n",
    "\n",
    "print(f\"\\n--- RQ2 분석용 테이블 (LongWiki, T_max={OPTIMAL_T_MAX} 기준) ---\")\n",
    "metrics_to_show_rq2 = ['Model', 'Method', 'factscore', 'Avg_Tokens']\n",
    "display(df_rq2[[col for col in metrics_to_show_rq2 if col in df_rq2.columns]].sort_values(by=['Model']))\n",
    "\n",
    "# --- RQ2 시각화 (예: FactScore vs Avg_Tokens) ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_rq2,\n",
    "    x='Avg_Tokens',\n",
    "    y='factscore',\n",
    "    hue='Model',\n",
    "    style='Method',\n",
    "    s=200 # 마커 크기\n",
    ")\n",
    "plt.title(f\"RQ2: 효율성 vs 정확성 (LongWiki, T_max={OPTIMAL_T_MAX})\")\n",
    "plt.xlabel(\"Average Tokens Used (Efficiency)\")\n",
    "plt.ylabel(\"FactScore (Accuracy)\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81fd670",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# RQ3 (Iterative vs 1-pass)\n",
    "# - 비교 대상: SERC (Iterative) vs SERC (1-pass)\n",
    "# - 데이터셋: HalluLens (LongWiki)\n",
    "# - Max_Facts는 동일한 값 (e.g., 최적 max_facts)으로 필터링\n",
    "\n",
    "OPTIMAL_MAX_FACTS = 5 # 예비 실험에서 결정된 값\n",
    "\n",
    "methods_to_compare_rq3 = [\"SERC (Iterative)\", \"SERC (1-pass)\"]\n",
    "df_rq3 = df_final_results[\n",
    "    (df_final_results['Method'].isin(methods_to_compare_rq3)) &\n",
    "    (df_final_results['Dataset'] == 'hallulens_longwiki') & # LongWiki 벤치마크\n",
    "    (df_final_results['Max_Facts'] == OPTIMAL_MAX_FACTS) # 동일 Max_Facts\n",
    "].copy()\n",
    "\n",
    "# (SERC (Iterative)는 T_max=OPTIMAL_T_MAX, SERC (1-pass)는 T_max=1 이어야 함)\n",
    "df_rq3 = df_rq3[\n",
    "    ((df_rq3['Method'] == 'SERC (1-pass)') & (df_rq3['T_max'] == 1)) |\n",
    "    ((df_rq3['Method'] == 'SERC (Iterative)') & (df_rq3['T_max'] == OPTIMAL_T_MAX))\n",
    "]\n",
    "\n",
    "print(f\"\\n--- RQ3 분석용 테이블 (LongWiki, Max_Facts={OPTIMAL_MAX_FACTS} 기준) ---\")\n",
    "metrics_to_show_rq3 = ['Model', 'Method', 'T_max', 'factscore']\n",
    "display(df_rq3[[col for col in metrics_to_show_rq3 if col in df_rq3.columns]].sort_values(by=['Model']))\n",
    "\n",
    "# --- RQ3 시각화 ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=df_rq3,\n",
    "    x='Model',\n",
    "    y='factscore',\n",
    "    hue='Method'\n",
    ")\n",
    "plt.title(f\"RQ3: 반복(Iterative) vs 1-Pass 효과 (LongWiki, Max_Facts={OPTIMAL_MAX_FACTS})\")\n",
    "plt.ylabel(\"FactScore\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xticks(rotation=15)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
