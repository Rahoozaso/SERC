import argparse
import os
import sys
import logging
import re
from typing import Dict, Any, List, Optional
from collections import defaultdict
from tqdm import tqdm

# --- Project path setup ---
PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
sys.path.append(PROJECT_ROOT)
sys.path.append(os.path.join(PROJECT_ROOT, "src"))

try:
    from src import programmatic_helpers as ph
    from src.utils import load_config, save_jsonl, get_timestamp
    from src.data_loader import load_dataset
    from src.model_wrappers import generate
    # RAGRetrieverëŠ” import í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (Ablation)

    from src.prompts import (
        BASELINE_PROMPT_TEMPLATE_PN,
        EXTRACT_FACTS_TEMPLATE_PN,
        generate_sentence_group_question_prompt, # ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ import
        VERIFICATION_ANSWER_TEMPLATE,            # ë‚´ë¶€ ë‹µë³€ ìƒì„±ìš©
        VALIDATE_EVIDENCE_TEMPLATE,              # 1:1 ê²€ì¦ìš©
        BP_CORRECTION_TEMPLATE,                  # BP ìˆ˜ì •ìš©
        RECONSTRUCT_LOCAL_SENTENCE_TEMPLATE,
        GLOBAL_POLISH_TEMPLATE,
    )
except ImportError as e:
    logging.error(f"ImportError: {e}. Check your src/ folder and PYTHONPATH.")
    sys.exit(1)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# =============================================================================
# Helper Functions: String & Formatting
# =============================================================================

def _extract_xml_tag(text: str, tag: str) -> str:
    if not text: return ""
    pattern = f"<{tag}>(.*?)</{tag}>"
    match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)
    if match: return match.group(1).strip()
    return ""

def _clean_model_output(raw: str) -> str:
    if not raw: return ""
    if "</" in raw: raw = raw.split("</")[0]
    stop_patterns = ["[END", "[/FINAL", "[ANSWER", "[SOLUTION"]
    for pat in stop_patterns:
        if pat in raw:
            if raw.find(pat) > 5: raw = raw.split(pat)[0]
    return re.sub(r'#.*$', '', raw, flags=re.MULTILINE).strip().strip('"').strip("'")

# =============================================================================
# Helper Functions: LLM Prompts
# =============================================================================

def prompt_baseline(query: str, model_name: str, config: dict) -> str:
    prompt = BASELINE_PROMPT_TEMPLATE_PN.format(query=query)
    return generate(prompt, model_name, config)

def prompt_extract_facts_from_sentence(sentence: str, model_name: str, config: dict, main_subject: str) -> List[str]:
    prompt = EXTRACT_FACTS_TEMPLATE_PN.format(sentence=sentence, main_subject=main_subject)
    raw = generate(prompt, model_name, config)
    facts = re.findall(r"<fact>(.*?)</fact>", raw, re.DOTALL | re.IGNORECASE)
    facts = [f.strip() for f in facts if f.strip()]
    if not facts:
        facts = [line[2:].strip() for line in raw.split('\n') if line.strip().startswith('- ')]
    return facts

def _prompt_generate_question_for_sentence_group(facts: List[str], model_name: str, config: dict, main_subject: str) -> str:
    # src/prompts.pyì— ìˆëŠ” í•¨ìˆ˜ ì‚¬ìš©
    prompt = generate_sentence_group_question_prompt(facts) 
    raw = generate(prompt, model_name, config)
    q = _extract_xml_tag(raw, "query")
    return q if q else f"{_clean_model_output(raw)} {main_subject}"

def prompt_reconstruct_local_sentence(original_sentence: str, updated_facts: List[str],
                                      query: str, model_name: str, config: dict) -> str:
    fact_list_str = "\n".join(f"- {f}" for f in updated_facts)
    prompt = RECONSTRUCT_LOCAL_SENTENCE_TEMPLATE.format(
        original_sentence=original_sentence,
        updated_facts=fact_list_str
    )
    raw = generate(prompt, model_name, config,
                   generation_params_override={"temperature": 0.3, "max_new_tokens": 512})
    # XML íƒœê·¸ ê°•ì œ ë¶€ì°© í›„ ì¶”ì¶œ
    modified_raw = f"<generated_sentence>{raw}"
    return _extract_xml_tag(modified_raw, "generated_sentence") or _clean_model_output(modified_raw)

def prompt_global_polish(query: str, draft_text: str, model_name: str, config: dict) -> str:
    prompt = GLOBAL_POLISH_TEMPLATE.format(query=query, draft_text=draft_text)
    raw = generate(prompt, model_name, config,
                   generation_params_override={"temperature": 0.5, "max_new_tokens": 1024})
    modified_raw = f"<final_response>{raw}"
    return _extract_xml_tag(modified_raw, "final_response") or _clean_model_output(modified_raw)

# =============================================================================
# Ablation Processing Functions (No RAG)
# =============================================================================

def _detect_syndromes_batch_no_rag(sentence_batches: List[Dict], 
                                   model_name: str, 
                                   config: Dict,
                                   main_subject: str) -> Dict[str, Any]:
    """
    [Ablation] Self-Verification (No RAG)
    1. ì§ˆë¬¸ ìƒì„±
    2. ë‚´ë¶€ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„± (Internal Evidence) -> VERIFICATION_ANSWER_TEMPLATE ì‚¬ìš©
    3. 1:1 ëŒ€ì¡° ë° ê²€ì¦
    """
    clean_facts = []
    syndromes_buffer = []

    logging.info(">>> [Ablation Step 1] Self-Verification Started (Internal Knowledge Only)")
    
    for batch in tqdm(sentence_batches, desc="Detecting (No RAG)"):
        facts = batch["original_facts"]
        if not facts: continue

        # 1. ê²€ì¦ ì§ˆë¬¸ ìƒì„±
        search_q = _prompt_generate_question_for_sentence_group(facts, model_name, config, main_subject)

        # 2. [í•µì‹¬] ë‚´ë¶€ ì§€ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„± (Internal Evidence Generation)
        # RAGRetriever ëŒ€ì‹ , LLMì—ê²Œ ì§ˆë¬¸ì„ ë˜ì ¸ì„œ ê¸°ì–µì„ ë„ì§‘ì–´ëƒ…ë‹ˆë‹¤.
        prompt_internal = VERIFICATION_ANSWER_TEMPLATE.format(question=search_q)
        internal_evidence = generate(prompt_internal, model_name, config)
        
        # 3. 1:1 ê²€ì¦ (Internal Evidence vs Fact)
        for fact in facts:
            # ê²€ì¦ í”„ë¡¬í”„íŠ¸ (ê¸°ì¡´ VALIDATE_EVIDENCE_TEMPLATE ì‚¬ìš©)
            # ì—¬ê¸°ì„œ evidence_text ìë¦¬ì— internal_evidenceê°€ ë“¤ì–´ê°‘ë‹ˆë‹¤.
            prompt_verify = VALIDATE_EVIDENCE_TEMPLATE.format(
                fact_text=fact, 
                evidence_text=internal_evidence
            )
            raw_output = generate(prompt_verify, model_name, config)
            
            verdict = _extract_xml_tag(raw_output, "judgment").upper()
            if not verdict: 
                verdict = "CONTRADICTED" if "CONTRADICTED" in raw_output.upper() else "SUPPORTED"

            if "SUPPORTED" in verdict:
                clean_facts.append(fact)
            else:
                # ì˜¤ë¥˜ë¡œ íŒë‹¨
                error_package = {
                    "original_fact": fact,  
                    "evidence": internal_evidence, # ì™¸ë¶€ ë¬¸ì„œ ëŒ€ì‹  ë‚´ë¶€ ì§€ì‹ì„ ì¦ê±°ë¡œ ì €ì¥
                    "context": internal_evidence,  # ìˆ˜ì • ë‹¨ê³„ì—ì„œë„ ì´ ë‚´ë¶€ ì§€ì‹ì„ contextë¡œ ì”€
                    "origin_sentence": batch["sentence"]
                }
                syndromes_buffer.append(error_package)
                logging.info(f"ğŸš« Self-Detected Error: {fact[:30]}... (vs Internal Belief)")
    
    return {
        "clean_facts": clean_facts,
        "syndromes_buffer": syndromes_buffer
    }

def _correct_syndromes_batch_no_rag(syndromes_buffer: List[Dict], 
                                    model_name: str, 
                                    config: Dict) -> Dict[str, str]:
    """
    [Ablation] Self-Correction with BP (No RAG)
    ì™¸ë¶€ ê²€ìƒ‰ ì—†ì´ ë‚´ë¶€ ì§€ì‹(Internal Evidence)ì„ Contextë¡œ ì‚¬ìš©í•˜ì—¬ ì—°ì‡„ ìˆ˜ì •í•©ë‹ˆë‹¤.
    """
    fact_correction_map = {}
    if not syndromes_buffer:
        return {}

    # 1. ë¬¸ì¥ë³„ë¡œ ì˜¤ë¥˜ ê·¸ë£¹í™” (BP ì ìš©)
    error_groups = defaultdict(list)
    for item in syndromes_buffer:
        error_groups[item["origin_sentence"]].append(item)

    logging.info(f">>> [Ablation Step 2] Self-Correction Started ({len(error_groups)} groups)")

    for sentence, items in tqdm(error_groups.items(), desc="Correcting (No RAG)"):
        # ì»¨í…ìŠ¤íŠ¸ëŠ” ê·¸ë£¹ ë‚´ ì²« ë²ˆì§¸ ê²ƒ ì‚¬ìš© (ë‚´ë¶€ ì§€ì‹)
        context = items[0]["context"]

        # ì…ë ¥ ë¸”ë¡ ìƒì„±
        error_block = ""
        for i, item in enumerate(items, 1):
            error_block += f"{i}. {item['original_fact']}\n"
        
        # XML BP í”„ë¡¬í”„íŠ¸ í˜¸ì¶œ (Mainê³¼ ë™ì¼í•œ í…œí”Œë¦¿ ì‚¬ìš©)
        prompt = BP_CORRECTION_TEMPLATE.format(
            context=context,
            error_block=error_block
        )
        
        raw_output = generate(prompt, model_name, config)
        
        # XML íŒŒì‹±
        correction_blocks = re.findall(r"<correction>(.*?)</correction>", raw_output, re.DOTALL | re.IGNORECASE)
        
        for block in correction_blocks:
            orig_match = re.search(r"<original>(.*?)</original>", block, re.DOTALL | re.IGNORECASE)
            fixed_match = re.search(r"<fixed>(.*?)</fixed>", block, re.DOTALL | re.IGNORECASE)
            
            if orig_match and fixed_match:
                clean_orig = re.sub(r'^[\d\-\.\)\s]+', '', orig_match.group(1).strip().strip("-").strip())
                clean_corr = fixed_match.group(1).strip()
                
                if clean_orig and clean_corr:
                    fact_correction_map[clean_orig] = clean_corr
                    logging.info(f"ğŸ”— Self-Fixed: {clean_orig[:15]}... -> {clean_corr[:15]}...")
            
    return fact_correction_map

# =============================================================================
# Main SERC Loop (No RAG Version)
# =============================================================================

def SERC_NO_RAG(query: str, model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:

    logging.info(f"--- SERC (Ablation: No RAG) Started --- Query: '{query[:60]}...'")
    
    history = {"query": query, "model_name": model_name, "steps": {}}

    # Step 1: Baseline Generation
    baseline = prompt_baseline(query, model_name, config)
    
    # Refusal Check: RAGê°€ ì—†ìœ¼ë¯€ë¡œ Cold Start ë¶ˆê°€ëŠ¥. ê±°ì ˆí•˜ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜.
    is_refusal = (
        len(baseline) < 50 or 
        "sorry" in baseline.lower() or 
        "cannot answer" in baseline.lower() or
        "don't have information" in baseline.lower()
    )
    if is_refusal:
        logging.warning("Baseline refused. Since this is No-RAG mode, we cannot perform Cold Start.")
        return {"query": query, "final_output": baseline, "status": "refusal_no_rag"}

    history["initial_baseline"] = baseline

    # Step 2: Fact Extraction
    sentences = ph.programmatic_split_into_sentences(baseline)
    sentence_batches = []
    
    # Entity Extraction (No RAG, so use Baseline Entity or Query)
    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•˜ê²Œ ì¿¼ë¦¬ ì „ì²´ë¥¼ ì£¼ì œë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ Baseline Entity ì¶”ì¶œ
    main_subject = query # Ablationì—ì„œëŠ” ë‹¨ìˆœí™”

    for s in sentences:
        if not s.strip(): continue
        facts = prompt_extract_facts_from_sentence(s, model_name, config, main_subject=main_subject)
        facts = [f for f in facts if len(f) > 5]
        if facts:
            sentence_batches.append({"sentence": s, "original_facts": facts})

    history["steps"]["sentence_batches"] = sentence_batches

    # Step 3: Detection (Self-Verification)
    detection_result = _detect_syndromes_batch_no_rag(
        sentence_batches=sentence_batches,
        model_name=model_name,
        config=config,
        main_subject=main_subject
    )
    
    clean_facts = detection_result["clean_facts"]
    syndromes_buffer = detection_result["syndromes_buffer"]

    # Step 4: Correction (Self-Correction BP)
    fact_correction_map = _correct_syndromes_batch_no_rag(
        syndromes_buffer=syndromes_buffer,
        model_name=model_name,
        config=config
    )

    history["steps"]["syndromes_detected"] = len(syndromes_buffer)
    history["steps"]["fact_correction_map"] = fact_correction_map

    # Step 5: Reconstruction (Conditional Zero-Base)
    logging.info("--- Reconstruction (No RAG) ---")
    local_sentences = []

    for batch in sentence_batches:
        orig_sent = batch["sentence"]
        old_facts = batch["original_facts"]
        
        updated_facts_list = []
        has_changes = False  # ë³€ê²½ ê°ì§€ í”Œë˜ê·¸
        
        for f in old_facts:
            if f in fact_correction_map:
                updated_facts_list.append(fact_correction_map[f])
                has_changes = True
            else:
                updated_facts_list.append(f)
        
        # ë³€ê²½ ì—†ìœ¼ë©´ Skip
        if not has_changes:
            local_sentences.append(orig_sent)
            continue
            
        # ë³€ê²½ ìˆìœ¼ë©´ ìƒˆë¡œ ìƒì„±
        reconstructed = prompt_reconstruct_local_sentence(
            original_sentence=orig_sent,
            updated_facts=updated_facts_list,
            query=query,
            model_name=model_name,
            config=config
        )
        final_sent = reconstructed.strip() if reconstructed and len(reconstructed) > 10 else orig_sent
        local_sentences.append(final_sent)

    # Step 6: Global Polish
    draft = "\n\n".join(local_sentences)
    if len(draft.strip()) < 50:
        final_output = baseline
    else:
        final_output = prompt_global_polish(query=query, draft_text=draft, model_name=model_name, config=config).strip()
    
    history["final_output"] = final_output
    return history

# =============================================================================
# Execution Wrapper
# =============================================================================

def run_single_item(item: Dict[str, Any], model_name: str, config: Dict[str, Any]) -> Dict[str, Any]:
    q = item.get("question") or item.get("query")
    try:
        result = SERC_NO_RAG(query=q, model_name=model_name, config=config)
        return {**item, "method_result": {"final_output": result["final_output"], "history": result, "status": "success"}}
    except Exception as e:
        logger.error(f"Error: {e}", exc_info=True)
        return {**item, "method_result": {"error": str(e), "status": "error"}}

def main():
    parser = argparse.ArgumentParser(description="SERC Ablation: No RAG")
    parser.add_argument("--config", type=str, default="config.yaml")
    parser.add_argument("--model", type=str, required=True)
    parser.add_argument("--dataset", type=str, required=True)
    parser.add_argument("--start", type=int, default=0)
    parser.add_argument("--end", type=int, default=None)
    parser.add_argument("--save_interval", type=int, default=10)
    parser.add_argument("--output_dir", type=str, default="results/serc_no_rag")
    args = parser.parse_args()

    config = load_config(args.config)
    data_path = os.path.join(PROJECT_ROOT, config['data_paths'][args.dataset])
    data = load_dataset(args.dataset, data_path)
    data = data[args.start: args.end]

    timestamp = get_timestamp()
    os.makedirs(os.path.join(PROJECT_ROOT, args.output_dir, args.model.replace('/', '_'), args.dataset), exist_ok=True)
    output_path = os.path.join(PROJECT_ROOT, args.output_dir, args.model.replace('/', '_'), args.dataset,
                               f"serc_no_rag_{args.start}-{len(data)+args.start}_{timestamp}.jsonl")

    results = []
    for i, item in enumerate(tqdm(data, desc="SERC No-RAG Processing")):
        results.append(run_single_item(item, args.model, config))
        if (i + 1) % args.save_interval == 0:
            save_jsonl(results, output_path)

    save_jsonl(results, output_path)
    logging.info(f"Done. Results â†’ {output_path}")

if __name__ == "__main__":
    main()