import argparse
import os
import sys
import logging
from typing import Dict, Any, List, Optional, Callable
import pprint
import re

# --- [1] í”„ë¡œì íŠ¸ ê²½ë¡œ ì„¤ì • ---
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(PROJECT_ROOT)

try:
    from src import programmatic_helpers as ph
    from src.utils import load_config, save_jsonl, get_timestamp
    from src.data_loader import load_dataset
    
    from src import prompts 
    from src.prompts import (
        generate_sentence_group_question_prompt,
        VERIFICATION_ANSWER_TEMPLATE,
        RECOMPOSE_PROMPT_TEMPLATE  # <--- [ì‹ ê·œ] 3.7ë‹¨ê³„ë¥¼ ìœ„í•œ í…œí”Œë¦¿ ì„í¬íŠ¸
    )
    from src.model_wrappers import generate 
    
except ImportError:
    logging.error("ImportError: 'src' í´ë” ë‚´ ëª¨ë“ˆ ì„í¬íŠ¸ ì‹¤íŒ¨. PYTHONPATHë¥¼ í™•ì¸í•˜ì„¸ìš”.")
    logging.error(f"PROJECT_ROOT: {PROJECT_ROOT}")
    logging.error(f"sys.path: {sys.path}")
    sys.exit(1)


# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- [2] `run_experiment.py`ê°€ ì„í¬íŠ¸í•  í—¬í¼ í•¨ìˆ˜ë“¤ ---

def prompt_baseline(query: str, model_name: str, config: dict) -> str:
    """Step 1: Generate Initial Response (Public)"""
    prompt = prompts.BASELINE_PROMPT_TEMPLATE.format(query=query)
    return generate(prompt, model_name, config)

def prompt_extract_facts_from_sentence(sentence: str, model_name: str, config: dict) -> str:
    """Step 2: Extract Facts from a Sentence (Internal)"""
    prompt = prompts.EXTRACT_FACTS_TEMPLATE.format(sentence=sentence)
    return generate(prompt, model_name, config)

def prompt_validate_one_fact_against_evidence(fact_text: str, evidence_text: str, model_name: str, config: dict) -> str:
    prompt = prompts.VALIDATE_EVIDENCE_TEMPLATE.format(fact_text=fact_text, evidence_text=evidence_text)
    response = generate(prompt, model_name, config)
    
    cleaned_response = response.strip().lower() if response else ""
    
    # [Yes] = ëª¨ìˆœ (ì‹ ë“œë¡¬ 1)
    if cleaned_response.startswith("[yes]") or cleaned_response.startswith("yes"):
        return "[Yes]"
    # [No] = ëª¨ìˆœ ì—†ìŒ (ì‹ ë“œë¡¬ 0)
    elif cleaned_response.startswith("[no]") or cleaned_response.startswith("no"):
        return "[No]"
    else:
        # (ë¡œê·¸ì—ì„œ ë°œê²¬ëœ ë¬¸ì œì ) ë¹ˆ ì¦ê±° ë“±ìœ¼ë¡œ ì¸í•´ [Yes]/[No] ì™¸ì˜ ì‘ë‹µì´ ì˜¤ë©´, 
        # ì´ë¥¼ [Yes]ë¡œ ê°„ì£¼í•˜ë©´ False Positiveê°€ ë°œìƒí•¨.
        # ë”°ë¼ì„œ, ëª…ë°±í•œ ëª¨ìˆœ([Yes])ì´ ì•„ë‹ˆë©´ [No](ëª¨ìˆœ ì—†ìŒ)ë¡œ ê°„ì£¼í•˜ëŠ” ê²ƒì´ ë” ì•ˆì „í•¨.
        logging.warning(f"Unexpected validation response: '{response}'. Defaulting to '[No]' (No Syndrome).")
        return "[No]" 

def _clean_model_output(raw_response: str) -> str:
    if not raw_response:
        return ""

    def _final_scrub(line: str) -> str:
        """V3.1ì˜ í•µì‹¬: ë¬¸ì¥ ëì— ë¶™ì€ ì¸ë¼ì¸ ì“°ë ˆê¸°ë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        # 1. # (í•´ì‹œ)ë¡œ ì‹œì‘í•˜ëŠ” ì£¼ì„/ì•µë¬´ìƒˆ ì œê±°
        line = re.sub(r'#.*$', '', line).strip()
        # 2. [...]ë¡œ ëë‚˜ëŠ” ê¼¬ë¦¬í‘œ íƒœê·¸ ì œê±°
        line = re.sub(r'\[.*?\]$', '', line).strip()
        # 3. ê·¸ ì™¸ í”„ë¡¬í”„íŠ¸ ì°Œêº¼ê¸°
        line = re.sub(r'END OF INSTRUCTION.*$', '', line, flags=re.IGNORECASE).strip()
        line = re.sub(r'Note:.*$', '', line, flags=re.IGNORECASE).strip()
        return line.strip().strip('"').strip("'")

    # --- 1. [ANSWER] íƒœê·¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ "ëª…ì‹œì  ë‹µë³€" ìš°ì„  ì¶”ì¶œ ---
    answer_markers = [r'\[ANSWER\]', r'Answer:', r'\[FINAL ANSWER\]', r'\[Final Answer\]:']
    for marker_pattern in answer_markers:
        match = re.search(marker_pattern + r'(.*)', raw_response, re.DOTALL | re.IGNORECASE)
        if match:
            potential_answer_block = match.group(1).strip()
            for line in potential_answer_block.splitlines():
                clean_line = line.strip()
                if len(clean_line) > 5 and not clean_line.startswith(('#', '|', '`', '_', '?', '[')):
                    final_answer = _final_scrub(clean_line) # <--- [V3.1] ì¸ë¼ì¸ ì²­ì†Œ ì ìš©
                    if final_answer:
                        logging.debug(f"[_clean_model_output] [ANSWER] ë§ˆì»¤ë¡œ ì¶”ì¶œ: '{final_answer}'")
                        return final_answer
            # [ANSWER] ë’¤ì— ì“¸ë§Œí•œ ê²Œ ì—†ìœ¼ë©´ 2ë‹¨ê³„ë¡œ ë„˜ì–´ê°

    # --- 2. [ANSWER] ë§ˆì»¤ê°€ ì—†ê±°ë‚˜ ì‹¤íŒ¨ ì‹œ, "ì „ì²´ í…ìŠ¤íŠ¸"ì—ì„œ ì“°ë ˆê¸° ì²­ì†Œ ---
    clean_text = raw_response
    patterns_to_remove = [
        r'\[.*?\]',
        r'\(Note:.*?\)',
        r'\(This statement is TRUE\.\)',
        r'(Step \d+:|Note:|REASONING|JUSTIFICATION|EXPLANATION|\[REASON\]|\[RATING\])',
        r'^\s*#+.*$',
        r'```python.*$',
        r'```'
        # [ì‹ ê·œ] Cycle 1, f5(í•™ë ¥) ìˆ˜ì • ì‹¤íŒ¨ ì‹œ ë°˜í™˜ëœ ì •í¬
        r'^\s*=+$', 
        # [ì‹ ê·œ] Cycle 1, f11(ê²°í˜¼) ìˆ˜ì • ì‹¤íŒ¨ ì‹œ ë°˜í™˜ëœ ì •í¬
        r'Identify the key elements in the corrected fact that need to be incorporated into the original sentence\.',
        # [ì‹ ê·œ] Cycle 1, f3(ê°€ì¡±) ìˆ˜ì • ì‹¤íŒ¨ ì‹œ ë°˜í™˜ëœ ì •í¬
        r'^\s*\(Choose one of the corrected facts\)\s*$',
        # [ì‹ ê·œ] Cycle 2, f3(ìƒì¼) ìˆ˜ì • ì‹¤íŒ¨ ì‹œ ë°˜í™˜ëœ ì •í¬
        r'The final answer is: _______________________________________\. \(Answer\)',
        # [ì‹ ê·œ] Cycle 2, f17(ì¹­í˜¸) ìˆ˜ì • ì‹¤íŒ¨ ì‹œ ë°˜í™˜ëœ ì •í¬
        r'I need help with this task! Can you provide the correct information\?',
        r'^\s*\(\s*$', # f17ì—ì„œ ë°˜í™˜ëœ ì™¸ë¡œìš´ ê´„í˜¸
        # [ì‹ ê·œ] Cycle 2, 'Forrest Gump' í™˜ê° ìœ ë°œ ë¬¸ì¥
        r'The movie, released in 1994, needs to be verified for accuracy\.'
    ]
    for pattern in patterns_to_remove:
        clean_text = re.sub(pattern, '', clean_text, flags=re.IGNORECASE | re.MULTILINE)

    clean_text = re.sub(r'^[\s|?_*#-]*$', '', clean_text, flags=re.MULTILINE)

    # --- 3. ì²« ë²ˆì§¸ 'ìœ ì˜ë¯¸í•œ' ì¤„ ì°¾ê¸° ---
    lines = [line.strip() for line in clean_text.splitlines()]
    for line in lines:
        if len(line) > 5 and not line.startswith(('_', '?', '|', '#', '`')):
            final_answer = _final_scrub(line) # <--- [V3.1] ì¸ë¼ì¸ ì²­ì†Œ ì ìš©
            if final_answer:
                logging.debug(f"[_clean_model_output] ì“°ë ˆê¸° í•„í„°ë§ í›„ ì²« ì¤„ ì¶”ì¶œ: '{final_answer}'")
                return final_answer

    logging.warning(f"[_clean_model_output] ëª¨ë¸ ì¶œë ¥ì´ ì“°ë ˆê¸°(garbage)ë¼ì„œ ëª¨ë‘ í•„í„°ë§ë¨. ì›ë³¸: '{raw_response[:100]}...'")
    return ""
def prompt_find_sentence(current_baseline: str, fact_text: str, model_name: str, config: dict) -> str:
    prompt = prompts.FIND_SENTENCE_TEMPLATE.format(current_baseline=current_baseline, 
        fact_text=fact_text)
    raw_response = generate(prompt, model_name, config)
    return _clean_model_output(raw_response)

def prompt_generate_correct_fact(fact_text: str, model_name: str, config: dict) -> str:
    prompt = prompts.CORRECT_FACT_TEMPLATE.format(fact_text=fact_text)
    raw_response = generate(prompt, model_name, config)
    return _clean_model_output(raw_response) 

def prompt_rewrite_sentence(bad_sentence: str, correct_fact_text: str, model_name: str, config: dict) -> str:
    prompt = prompts.REWRITE_SENTENCE_TEMPLATE.format(bad_sentence=bad_sentence, 
        correct_fact_text=correct_fact_text)
    raw_response = generate(prompt, model_name, config)
    return _clean_model_output(raw_response) 


# --- [ì‹ ê·œ] 3.7 ìµœì¢… ì¬êµ¬ì„± í—¬í¼ í•¨ìˆ˜ ---
def prompt_recompose(query: str, final_facts_map: Dict[str, str], model_name: str, config: dict) -> str:
    """Step 3.7: Final Recomposition (New)"""
    
    # {f_id: text} ë§µì—ì„œ text ê°’ë“¤ë§Œ ì¶”ì¶œí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    fact_texts = [f"- {text}" for text in final_facts_map.values() if text and len(text) > 5]
    if not fact_texts:
        logging.warning("[prompt_recompose] ì¬êµ¬ì„±ì„ ìœ„í•œ ìœ ì˜ë¯¸í•œ ì‚¬ì‹¤ ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        return "N/A"
        
    fact_list_str = "\n".join(fact_texts)
    
    prompt = prompts.RECOMPOSE_PROMPT_TEMPLATE.format(
        query=query,
        fact_list_str=fact_list_str
    )
    
    # ì¬êµ¬ì„±ì€ ì•½ê°„ì˜ ì°½ì˜ì„±ì´ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‚˜, í™˜ê°ì„ ë§‰ê¸° ìœ„í•´ temperatureëŠ” ë‚®ê²Œ ìœ ì§€
    
    raw_response = generate(prompt, model_name, config)
    
    # ì¬êµ¬ì„± í”„ë¡¬í”„íŠ¸ëŠ” [Final Answer] íƒœê·¸ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ, _clean_model_outputì´ ì˜ ì‘ë™í•¨
    return _clean_model_output(raw_response)


# --- [3] Fact-in-Sentence ë¡œì§ì˜ ë‚´ë¶€ í—¬í¼ í•¨ìˆ˜ ---

def _prompt_generate_question_for_sentence_group(fact_texts_list: List[str], model_name: str, config: dict) -> str:
    """ 3a. QG (Internal Helper) """
    prompt = generate_sentence_group_question_prompt(fact_texts_list)
    question_params = {"temperature": 0.01, "max_new_tokens": 75}
    raw_response = generate(prompt, model_name, config, generation_params_override=question_params)
    clean_text = raw_response
    
    hallucination_tags = [
        "[SENTENCE]", "[INSTRUCTION]", "[ANSWER]", "[REASON]",
        "[VERIFICATION]", "(Note:", "The final answer is:"
    ]
    
    indices = []
    for tag in hallucination_tags:
        idx = clean_text.find(tag)
        if idx != -1:
            indices.append(idx)
    
    split_idx = min(indices) if indices else -1
    
    if split_idx != -1:
        clean_text = clean_text[:split_idx]

    question_mark_index = clean_text.rfind('?')
    if question_mark_index != -1:
        clean_text = clean_text[:question_mark_index + 1]
        
    return clean_text.strip().strip('"').strip("'")



def _prompt_get_verification_answer(question: str, model_name: str, config: dict) -> str:
    """ 3b. AG (Internal Helper) """
    prompt = VERIFICATION_ANSWER_TEMPLATE.format(question=question)
    answer_params = {"temperature": 0.01, "max_new_tokens": 100}
    raw_response = generate(prompt, model_name, config, generation_params_override=answer_params)

    clean_text = raw_response

    hallucination_tags = [
        "[SENTENCE]", "[INSTRUCTION]", "[ANSWER]", "[REASON]",
        "[VERIFICATION]", "(Note:", "The final answer is:"
    ]
    
    indices = []
    for tag in hallucination_tags:
        idx = clean_text.find(tag)
        if idx != -1:
            indices.append(idx)
    
    split_idx = min(indices) if indices else -1
    
    if split_idx != -1:
        clean_text = clean_text[:split_idx]
    clean_text = clean_text.split('\n')[0]

    return clean_text.strip().strip('"').strip("'")


# --- [4] `run_experiment.py`ê°€ ì„í¬íŠ¸í•  ë©”ì¸ `SERC` í•¨ìˆ˜ ---

def SERC(query: str, model_name: str, config: Dict[str, Any],
         t_max: Optional[int] = None,
         max_facts_per_group: Optional[int] = None, # (ì´ì „ ì½”ë“œì—ì„œ ëˆ„ë½ëœ íŒŒë¼ë¯¸í„° ì¶”ê°€)
         ground_truth_eval: Optional[Any] = None,
         eval_func: Optional[Callable] = None,
         return_intermediate_results: bool = False
         ) -> Dict[str, Any]:

    # --- í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì • ---
    T_MAX = t_max if t_max is not None else config.get('default_t_max', 3)
    # (ì´ì „ ì½”ë“œì—ì„œ ëˆ„ë½ëœ íŒŒë¼ë¯¸í„° ì„¤ì • ë¡œì§ ì¶”ê°€)
    MAX_FACTS_PER_GROUP = max_facts_per_group if max_facts_per_group is not None else config.get('default_max_facts_per_group', 5)

    logging.info(f"--- SERC [Fact-in-Sentence] ì‹¤í–‰ ì‹œì‘ --- Query: '{query[:50]}...'")
    logging.info(f"Model: {model_name}, T_max: {T_MAX}, Max_Facts_Per_Group: {MAX_FACTS_PER_GROUP}")

    history = {'query': query, 'model_name': model_name, 'params': {'t_max': T_MAX, 'method': 'fact-in-sentence', 'max_facts': MAX_FACTS_PER_GROUP}, 'cycles': []}

    # --- 1. ì´ˆê¸° ë‹µë³€ ìƒì„± ---
    logging.info("--- [1ë‹¨ê³„] ì´ˆê¸° ë‹µë³€ ìƒì„± ---")
    current_baseline = prompt_baseline(query, model_name, config)
    history['initial_baseline'] = current_baseline
    logging.info(f" Â ì´ˆê¸° Baseline ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(current_baseline)}ì)")
    logging.debug(f" Â [1ë‹¨ê³„-ì „ì²´ ì¶œë ¥ë¬¼] \n{current_baseline}")
    
    try:
        initial_sentences = ph.programmatic_split_into_sentences(current_baseline)
        if len(initial_sentences) > 1:
            last_sentence = initial_sentences[-1].strip()
            if last_sentence and not last_sentence.endswith(('.', '?', '!', '"', "'", "â€", "â€™")):
                logging.warning(f" Â [1ë‹¨ê³„-í•„í„°ë§] ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ë¶ˆì™„ì „í•˜ì—¬ ì œê±°í•©ë‹ˆë‹¤: '{last_sentence}'")
                filtered_sentences = initial_sentences[:-1]
                current_baseline = " ".join(filtered_sentences).strip()
                logging.info(f" Â [1ë‹¨ê³„-í•„í„°ë§] í•„í„°ë§ëœ Baseline (ê¸¸ì´: {len(current_baseline)}ì)")
            else:
                logging.info(f" Â [1ë‹¨ê³„-í•„í„°ë§] ë§ˆì§€ë§‰ ë¬¸ì¥ì´ ì˜¨ì „í•˜ì—¬ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        elif len(initial_sentences) == 1:
            logging.info(f" Â [1ë‹¨ê³„-í•„í„°ë§] ë¬¸ì¥ì´ 1ê°œì´ë¯€ë¡œ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        else:
            logging.info(f" Â [1ë‹¨ê³„-í•„í„°ë§] ë¬¸ì¥ì´ ì—†ì–´ í•„í„°ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
    except Exception as e:
        logging.error(f" Â [1ë‹¨ê³„-í•„í„°ë§] ë§ˆì§€ë§‰ ë¬¸ì¥ í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
    
    
    total_cycles_executed = 0
    final_facts_map_from_last_cycle: Dict[str, str] = {} # [ì‹ ê·œ] 3.7ë‹¨ê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜

    # --- 2. ë°˜ë³µì  êµì • ë£¨í”„ ---
    for t in range(1, T_MAX + 1):
        total_cycles_executed = t
        cycle_log: Dict[str, Any] = {'cycle': t, 'steps': {}, 'baseline_before_cycle': current_baseline}
        logging.info(f"\n--- [ì‚¬ì´í´ {t}/{T_MAX}] êµì • ì‹œì‘ ---")

        # --- 2a. ì‚¬ì‹¤ ì¶”ì¶œ ---
        logging.info(" Â [2ë‹¨ê³„] ì‚¬ì‹¤ ì¶”ì¶œ ë° ë¬¸ì¥ ê·¸ë£¹í™” ì‹œì‘...")
        sentences = ph.programmatic_split_into_sentences(current_baseline)
        
        sentence_groups: List[Dict[str, Any]] = [] 
        all_facts: Dict[str, str] = {} 
        
        fact_id_counter = 1
        raw_extractions = []
        
        for s in sentences:
            if not s: continue
            
            raw_extracted_list_str = prompt_extract_facts_from_sentence(s, model_name, config)
            clean_text = raw_extracted_list_str
            
            marker1 = "[SENTENCE]"
            idx1 = clean_text.find(marker1)
            marker2 = "[INSTRUCTION]"
            idx2 = clean_text.find(marker2)
            indices = [i for i in [idx1, idx2] if i != -1]
            split_idx = min(indices) if indices else -1
            
            if split_idx != -1:
                clean_text = clean_text[:split_idx] 
                
            clean_extracted_list_str = clean_text.strip()
            
            raw_extractions.append({'sentence': s, 'extracted_str': raw_extracted_list_str}) 
            parsed_facts_list = ph.programmatic_parse_fact_list(clean_extracted_list_str) 
            
            if parsed_facts_list:
                sentence_facts_map = {}
                for fact_text in parsed_facts_list:
                    fid = f"f{fact_id_counter}"
                    fact_text = fact_text.strip()
                    sentence_facts_map[fid] = fact_text
                    all_facts[fid] = fact_text 
                    fact_id_counter += 1
                
                if sentence_facts_map:
                    sentence_groups.append({
                        'sentence': s,
                        'facts': sentence_facts_map 
                    })

        cycle_log['steps']['2_fact_extraction'] = {'raw': raw_extractions, 'sentence_groups': sentence_groups, 'all_facts_map': all_facts.copy()}
        
        # [ì‹ ê·œ] 3.7ë‹¨ê³„ë¥¼ ìœ„í•´, ì´ ì‚¬ì´í´ì—ì„œ ì¶”ì¶œëœ (ì ì¬ì ìœ¼ë¡œ ì˜¤ì—¼ëœ) ì‚¬ì‹¤ ë§µì„ ì €ì¥
        final_facts_map_from_last_cycle = all_facts.copy()

        if not all_facts:
            logging.info(" Â [2ë‹¨ê³„] ì¶”ì¶œëœ ì‚¬ì‹¤ ì—†ìŒ. ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            history['termination_reason'] = 'no_facts_extracted'
            break
        logging.info(f" Â [2ë‹¨ê³„] ì´ {len(all_facts)}ê°œ ì‚¬ì‹¤(ë³€ìˆ˜ ë…¸ë“œ) / {len(sentence_groups)}ê°œ ë¬¸ì¥(ê²€ì‚¬ ë…¸ë“œ) ì‹ë³„.")
        
        # --- 2b. ì‹ ë“œë¡¬ ìƒì„± ---
        logging.info(f" Â [3ë‹¨ê³„] {len(sentence_groups)}ê°œ ë¬¸ì¥ ê·¸ë£¹ ê²€ì¦ ì‹œì‘...")
        
        syndrome: Dict[str, Dict[str, str]] = {} 
        validation_details = []


        for group in sentence_groups:
            sentence_text = group['sentence']
            facts_in_group = group['facts']

            if not facts_in_group:
                continue

            fact_items_list = list(facts_in_group.items()) 

            # (ì´ì „ ì½”ë“œì˜ ì²­í‚¹ ë¡œì§ ëŒ€ì‹ , MAX_FACTS_PER_GROUPì„ ì‚¬ìš©í•˜ë„ë¡ ìˆ˜ì •)
            for i in range(0, len(fact_items_list), MAX_FACTS_PER_GROUP):
                chunk = fact_items_list[i : i + MAX_FACTS_PER_GROUP]
                
                fact_ids_chunk = [item[0] for item in chunk]
                fact_texts_chunk = [item[1] for item in chunk]

                logging.debug(f" Â  Â - ì²­í¬ ê²€ì¦: (ë¬¸ì¥: '{sentence_text[:30]}...', ì‚¬ì‹¤ {i+1}~{i+len(chunk)})")

                # (Model Call) 3a. 'ê·¸ë£¹(ë¬¸ì¥)' ì§ˆë¬¸ ìƒì„±
                q = _prompt_generate_question_for_sentence_group(
                    fact_texts_list=fact_texts_chunk, 
                    model_name=model_name, 
                    config=config
                )
                
                if q.strip().lower() == "none" or not q.strip():
                    logging.warning(f" Â  Â [ê²½ê³ ] ê·¸ë£¹ ì§ˆë¬¸ ìƒì„± ì‹¤íŒ¨. ê±´ë„ˆëœë‹ˆë‹¤.")
                    validation_details.append({'group_context': 'N/A', 'status': 'question_failed'})
                    continue
                    
                # (Model Call) 3b. ê²€ì¦ ë‹µë³€ ìƒì„±
                verified_answer = _prompt_get_verification_answer(q, model_name, config)
                
                # (Model Call) 3c. 1:1 íŒ¨ë¦¬í‹° ê²€ì‚¬
                for fid, ftext in chunk: 
                    validation_result = prompt_validate_one_fact_against_evidence(
                        ftext, verified_answer, model_name, config
                    )
                    
                    validation_details.append({
                        'fact_id': fid, 'fact_text': ftext,
                        'sentence': sentence_text, 'group_question': q, 
                        'verified_answer': verified_answer, 'result': validation_result
                    })

                    if validation_result == "[Yes]":
                        logging.info(f" Â  Â [!!! ì‹ ë“œë¡¬ íƒì§€ !!!] {fid}: {ftext}")
                        syndrome[fid] = {"fact_text": ftext, "evidence": verified_answer, "original_sentence": sentence_text}

        cycle_log['steps']['3_syndrome_generation'] = validation_details

        # --- 2c. ìˆ˜ë ´ í™•ì¸ ---
        if not syndrome:
            logging.info(f"\n Â [4cë‹¨ê³„] ì‹ ë“œë¡¬ ì—†ìŒ. ì‚¬ì´í´ {t}ì—ì„œ ìˆ˜ë ´.")
            history['termination_reason'] = f'converged_at_cycle_{t}'
            break
        else:
             logging.info(f"\n Â [4Cë‹¨ê³„] ì´ {len(syndrome)}ê°œì˜ ì˜¤ë¥˜ ì‚¬ì‹¤(ì‹ ë“œë¡¬) íƒì§€. êµì • ì‹œì‘.")

        # --- 2d. êµì • ---
        logging.info(" Â [5ë‹¨ê³„] ë¶„í•´ëœ êµì • ì ìš© ì‹œì‘...")
        facts_to_correct = syndrome
        final_response_snapshot = current_baseline 
        correction_log = []

        for fi, error_info in facts_to_correct.items():
            fi_text = error_info['fact_text']
            correction_item: Dict[str, Any] = {'fact_id': fi, 'original_fact': fi_text}
            logging.info(f" Â  Â - ì˜¤ë¥˜ {fi} êµì • ì‹œë„: '{fi_text[:100]}...'")

            # (ìˆ˜ì •) 5a. íƒìƒ‰ (prompt_find_sentence í˜¸ì¶œ ëŒ€ì‹ , ì‹ ë“œë¡¬ì— ì €ì¥ëœ 'original_sentence'ë¥¼ ì‚¬ìš©)
            bad_sentence = error_info.get('original_sentence', '').strip()

            correction_item['found_sentence'] = bad_sentence
            if not bad_sentence:
              logging.warning(f" Â  Â [ê²½ê³ ] ì˜¤ë¥˜ {fi} ì‹ ë“œë¡¬ì— ì›ë³¸ ë¬¸ì¥(original_sentence) ì—†ìŒ. êµì • ê±´ë„ˆëœë‹ˆë‹¤.")
              correction_item['status'] = 'find_failed_no_sentence_in_syndrome'
              correction_log.append(correction_item)
              continue

            # (ì‹ ê·œ) í˜„ì¬ baselineì— í•´ë‹¹ ë¬¸ì¥ì´ ì—†ìœ¼ë©´, ì´ì „ êµì •ì—ì„œ ë®ì–´ì“°ì¸ ê²ƒì´ë¯€ë¡œ ê±´ë„ˆëœ€
            if bad_sentence not in final_response_snapshot:
              logging.warning(f" Â  Â [ê²½ê³ ] ì˜¤ë¥˜ {fi}ì˜ ì›ë³¸ ë¬¸ì¥ì´ í˜„ì¬ baselineì— ì—†ìŒ. (ì´ì „ êµì •ì—ì„œ ë®ì–´ì“°ì¸ ë“¯ í•¨). êµì • ê±´ë„ˆëœë‹ˆë‹¤.")
              correction_item['status'] = 'find_failed_sentence_not_in_baseline'
              correction_log.append(correction_item)
              continue

            # (Model Call) 5b. ì‚¬ì‹¤ ìˆ˜ì •
            correct_fact_text = prompt_generate_correct_fact(fi_text, model_name, config)
            correction_item['corrected_fact'] = correct_fact_text
            if not correct_fact_text:
                logging.warning(f" Â  Â [ê²½ê³ ] ì˜¤ë¥˜ {fi} ìˆ˜ì •ëœ íŒ©íŠ¸ ìƒì„± ì‹¤íŒ¨. êµì • ê±´ë„ˆëœë‹ˆë‹¤.")
                correction_item['status'] = 'correct_fact_failed'
                correction_log.append(correction_item)
                continue
            
            # (Model Call) 5c. ë¬¸ì¥ ì¬ì‘ì„±
            good_sentence = prompt_rewrite_sentence(bad_sentence, correct_fact_text, model_name, config)
            correction_item['rewritten_sentence'] = good_sentence
            if not good_sentence:
                logging.warning(f" Â  Â [ê²½ê³ ] ì˜¤ë¥˜ {fi} ë¬¸ì¥ ì¬ì‘ì„± ì‹¤íŒ¨. êµì • ê±´ë„ˆëœë‹ˆë‹¤.")
                correction_item['status'] = 'rewrite_failed'
                correction_log.append(correction_item)
                continue
            
            # (ì‹ ê·œ) êµì • ê²°ê³¼ê°€ ì›ë³¸ê³¼ ë™ì¼í•˜ë©´, êµì²´í•˜ì§€ ì•Šê³  ë„˜ì–´ê°
            if bad_sentence == good_sentence:
              logging.info(f" Â  Â - ì˜¤ë¥˜ {fi} êµì • ê²°ê³¼ ì›ë³¸ê³¼ ë™ì¼. ë³€ê²½ ì—†ìŒ.")
              correction_item['status'] = 'corrected_no_change'
              correction_log.append(correction_item)
              continue

            # (Programmatic) 5d. ëŒ€ì²´
            temp_snapshot = ph.programmatic_replace(final_response_snapshot, bad_sentence, good_sentence)
            if temp_snapshot == final_response_snapshot:
                 logging.warning(f" Â  Â [ê²½ê³ ] ì˜¤ë¥˜ {fi} êµì • ìœ„í•œ ë¬¸ì¥ ëŒ€ì²´ ì‹¤íŒ¨.")
                 correction_item['status'] = 'replace_failed'
            else:
                final_response_snapshot = temp_snapshot
                correction_item['status'] = 'corrected'
                logging.info(f" Â  Â - ì˜¤ë¥˜ {fi} êµì • ì ìš© ì™„ë£Œ.")
            correction_log.append(correction_item)

        cycle_log['steps']['5_correction'] = correction_log
        current_baseline = final_response_snapshot
        logging.info(f" Â [5ë‹¨ê³„] ì‚¬ì´í´ {t} êµì • ì ìš© ì™„ë£Œ.")
        cycle_log['baseline_after_cycle'] = current_baseline
        history['cycles'].append(cycle_log)
    
    # --- [ì‹ ê·œ 3.7] 3ë‹¨ê³„: ì•µì»¤ë§ëœ ìµœì¢… ì¬êµ¬ì„± ---
    logging.info(f"\n--- [3.7ë‹¨ê³„] ìµœì¢… ì¬êµ¬ì„± ì‹œì‘ ---")
    dirty_final_baseline = current_baseline
    history['dirty_baseline_before_recomposition'] = current_baseline 
    logging.info(f" Â [3.7a] ë”ëŸ¬ìš´ Baselineì—ì„œ ìµœì¢… ì‚¬ì‹¤ ëª©ë¡ ì¶”ì¶œ...")
    final_sentences = ph.programmatic_split_into_sentences(dirty_final_baseline)
    final_facts_map_for_recomposition: Dict[str, str] = {}
    fid_counter = 1
    
    for s in final_sentences:
        if not s: continue
        # (2a ë‹¨ê³„ì˜ ë¡œì§ì„ ì—¬ê¸°ì„œ í•œ ë²ˆ ë” ìˆ˜í–‰)
        raw_extracted_list_str = prompt_extract_facts_from_sentence(s, model_name, config)
        
        clean_text = raw_extracted_list_str
        marker1 = "[SENTENCE]"; idx1 = clean_text.find(marker1)
        marker2 = "[INSTRUCTION]"; idx2 = clean_text.find(marker2)
        indices = [i for i in [idx1, idx2] if i != -1]
        split_idx = min(indices) if indices else -1
        if split_idx != -1:
            clean_text = clean_text[:split_idx]
        
        clean_extracted_list_str = clean_text.strip()
        parsed_facts_list = ph.programmatic_parse_fact_list(clean_extracted_list_str)
        
        for fact_text in parsed_facts_list:
            fid = f"reco_f{fid_counter}"
            fact_text = fact_text.strip()
            final_facts_map_for_recomposition[fid] = fact_text
            fid_counter += 1

    if not final_facts_map_for_recomposition:
        logging.warning(" Â [3.7] ì¬êµ¬ì„±ì„ ìœ„í•œ ì‚¬ì‹¤ ë§µì´ ì—†ìŠµë‹ˆë‹¤. êµì •ëœ Baselineì„ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.")
        clean_final_baseline = dirty_final_baseline # Fallback
    else:
        logging.info(f" Â [3.7b] ìµœì¢… ì¶”ì¶œëœ {len(final_facts_map_for_recomposition)}ê°œ ì‚¬ì‹¤ ë§µì„ ì‚¬ìš©í•˜ì—¬ ì¬êµ¬ì„± ì‹œì‘...") # <--- ğŸš¨ [B-2] "ì˜¬ë°”ë¥¸" ë§µì„ ì‚¬ìš©
        
        # [3.7b] ì•µì»¤ë§ëœ ì¬êµ¬ì„± í˜¸ì¶œ
        clean_final_baseline = prompt_recompose(
            query=query,
            final_facts_map=final_facts_map_for_recomposition, #
            model_name=model_name,
            config=config
        )
        
        if clean_final_baseline.strip().lower() == "n/a" or not clean_final_baseline.strip():
             logging.warning(f" Â [3.7b] ì¬êµ¬ì„± ì‹¤íŒ¨ (N/A ë°˜í™˜). êµì •ë³¸(Dirty)ìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
             clean_final_baseline = dirty_final_baseline # Fallback
        else:
             logging.info(" Â [3.7b] ì¬êµ¬ì„± ì„±ê³µ.")
    
    
    # --- ë£¨í”„ ì¢…ë£Œ í›„ ìµœì¢… ê²°ê³¼ ê¸°ë¡ ---
    history['final_baseline'] = clean_final_baseline # [!!! ì¤‘ìš” !!!] ìµœì¢… ì¶œë ¥ì„ 'ì •ì œëœ' ë²„ì „ìœ¼ë¡œ êµì²´
    history['total_cycles_executed'] = total_cycles_executed
    if 'termination_reason' not in history:
        history['termination_reason'] = f'max_iterations_reached (T={T_MAX})'
    logging.info(f"--- SERC [Fact-in-Sentence] ì‹¤í–‰ ì¢…ë£Œ (ì´ {total_cycles_executed} ì‚¬ì´í´) ---")
    
    return history


# --- [5] ëª¨ë“ˆ í…ŒìŠ¤íŠ¸ìš© ì‹¤í–‰ ë¸”ë¡ ---
if __name__ == "__main__":
    logging.info("--- [src/main_serc.py] ëª¨ë“ˆì„ ì§ì ‘ ì‹¤í–‰í•©ë‹ˆë‹¤ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ) ---")
    
    parser = argparse.ArgumentParser(description="Run SERC (Fact-in-Sentence) Experiment (Directly).")
    
    default_config_path = os.path.join(PROJECT_ROOT, "config.yaml")
    parser.add_argument("--config", type=str, default=default_config_path, help="Path to config file.")
    
    parser.add_argument("--model", type=str, required=True, help="Model name (defined in config).")
    parser.add_argument("--dataset", type=str, required=True, help="Dataset name (key in config data_paths).")
    parser.add_argument("--limit", type=int, default=None, help="Limit data points. Default: All")
    parser.add_argument("--output_dir", type=str, default="results/SERC", help="Dir to save results.")
    parser.add_argument("--output_suffix", type=str, default="", help="Optional output filename suffix.")
    
    parser.add_argument("--t_max", type=int, default=None, help="Override default T_max (runs iteratively up to this value).")
    # [ì‹ ê·œ] max_facts_per_group ì•„ê·œë¨¼íŠ¸ ì¶”ê°€
    parser.add_argument("--max_facts_per_group", type=int, default=None, help="Override default max_facts_per_group.")

    args = parser.parse_args()
    
    try:
        config = load_config(args.config)
    except FileNotFoundError:
        logger.error(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.config}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)
        
    T_MAX_TO_RUN = args.t_max if args.t_max is not None else config.get('default_t_max', 3)
    # [ì‹ ê·œ] max_facts_per_group ê°’ ì„¤ì •
    MAX_FACTS_TO_RUN = args.max_facts_per_group if args.max_facts_per_group is not None else config.get('default_max_facts_per_group', 5)


    logging.info(f"--- SERC (Fact-in-Sentence) [Direct Run] ì‹¤í—˜ ì‹œì‘ ---")
    logging.info(f"Config: {args.config}")
    logging.info(f"Model: {args.model}")
    logging.info(f"Dataset: {args.dataset} (Limit: {args.limit if args.limit is not None else 'All'})")
    logging.info(f"T_max: {T_MAX_TO_RUN}") 
    logging.info(f"Max_Facts_Per_Group: {MAX_FACTS_TO_RUN}") # [ì‹ ê·œ] ë¡œê·¸ ì¶”ê°€

    dataset_config_key = args.dataset
    relative_path = config.get('data_paths', {}).get(dataset_config_key)
    if not relative_path:
         logger.error(f"Config íŒŒì¼({args.config})ì˜ 'data_paths'ì—ì„œ '{dataset_config_key}' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
         sys.exit(1)
    dataset_path = os.path.join(PROJECT_ROOT, relative_path)
    
    try:
        data = load_dataset(dataset_config_key, dataset_path)
    except FileNotFoundError:
        logger.error(f"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {dataset_path}")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ë°ì´í„°ì…‹ ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ ({dataset_path}). ì¢…ë£Œí•©ë‹ˆë‹¤.", exc_info=True)
        sys.exit(1)
    
    if args.limit and args.limit > 0:
        if args.limit < len(data): data = data[:args.limit]
        logging.info(f"ë°ì´í„° {len(data)}ê°œë¡œ ì œí•œí•˜ì—¬ ì‹¤í–‰.")
    else:
        logging.info(f"ë°ì´í„°ì…‹ {len(data)}ê°œ ì „ì²´ ì‚¬ìš©.")
    if not data:
        logger.error("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹¤í—˜ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
        sys.exit(1)
        
    if not any(m['name'] == args.model for m in config.get('models', [])):
         logger.error(f"ì˜¤ë¥˜: ëª¨ë¸ '{args.model}'ì´(ê°€) ì„¤ì • íŒŒì¼ '{args.config}'ì— ì •ì˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
         sys.exit(1)

    timestamp = get_timestamp()
    results_base_dir_abs = os.path.join(PROJECT_ROOT, args.output_dir)
    results_dir = os.path.join(results_base_dir_abs, args.model.replace('/', '_'), args.dataset)
    
    limit_str = f"_limit{args.limit}" if args.limit else ""
    suffix_str = f"_{args.output_suffix}" if args.output_suffix else ""
    output_filename = f"serc_fact_in_sentence_t{T_MAX_TO_RUN}{limit_str}{suffix_str}_{timestamp}.jsonl"
    output_path = os.path.join(results_dir, output_filename)
    logging.info(f"ê²°ê³¼ëŠ” ë‹¤ìŒ ê²½ë¡œì— ì €ì¥ë©ë‹ˆë‹¤: {output_path}")

    results = []
    from tqdm import tqdm
    for item in tqdm(data, desc=f"SERC (Fact-in-Sentence, T={T_MAX_TO_RUN})"):
        try:
            serc_history = SERC(
                query=item.get('question', item.get('query')),
                model_name=args.model,
                config=config,
                t_max=T_MAX_TO_RUN,
                max_facts_per_group=MAX_FACTS_TO_RUN # [ì‹ ê·œ] íŒŒë¼ë¯¸í„° ì „ë‹¬
            )
            method_result = {'serc_result': serc_history, 'final_output': serc_history.get('final_baseline', ''), 'status': 'success'}
        except Exception as e:
            logger.error(f"'{item.get('query')}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (Fact-in-Sentence): {e}", exc_info=False)
            method_result = {"error": f"Exception during processing: {e}", "status": "error"}

        output_item = {
            **item, 
            "method_result": method_result,
            "method_used": f"serc_fact_in_sentence_t{T_MAX_TO_RUN}"
        }
        results.append(output_item)
    
    try:
        save_jsonl(results, output_path)
        logging.info(f"\n--- SERC (Fact-in-Sentence) [Direct Run] ì‹¤í—˜ ì™„ë£Œ. ì´ {len(results)}ê°œì˜ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ---")
    except Exception as e:
        logger.error(f"ìµœì¢… ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)