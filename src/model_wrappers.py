import os
from dotenv import load_dotenv
import time
import random
from typing import Dict, Any, Optional
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from src.utils import token_tracker

load_dotenv()
_loaded_models = {}

# --- Hugging Face í† í° ë¡œë“œ (í•„ìš”ì‹œ) ---
def _get_huggingface_token(config: Dict[str, Any]) -> Optional[str]:
    """í™˜ê²½ ë³€ìˆ˜ HF_TOKEN ë˜ëŠ” config íŒŒì¼ì—ì„œ Hugging Face í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    key_name = 'huggingface_token'
    env_var = 'HF_TOKEN' # Hugging Faceê°€ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ í™˜ê²½ ë³€ìˆ˜ ì´ë¦„
    token = os.environ.get(env_var)
    if token:
        print(f"Hugging Face í† í°ì„ í™˜ê²½ ë³€ìˆ˜ {env_var}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return token
    print(f"ê²½ê³ : Hugging Face í† í°ì„ í™˜ê²½ ë³€ìˆ˜ {env_var} ë˜ëŠ” configì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì ‘ê·¼ ì œí•œ ëª¨ë¸ ë¡œë”© ë¶ˆê°€)")
    return None

# --- ë©”ì¸ ìƒì„± í•¨ìˆ˜ ---
def generate(prompt: str, model_name: str, config: Dict[str, Any],
             generation_params_override: Optional[Dict[str, Any]] = None) -> str:
    
    print(f"\n--- ëª¨ë¸ í˜¸ì¶œ ì‹œì‘: {model_name} ---")
    print(f"í”„ë¡¬í”„íŠ¸ (ì‹œì‘):\n{prompt[:200]}...\n") 

    # ëª¨ë¸ ì„¤ì • ì°¾ê¸°
    model_config = next((m for m in config.get('models', []) if m.get('name') == model_name), None)
    if not model_config:
        print(f" ì˜¤ë¥˜: ëª¨ë¸ '{model_name}'ì„(ë¥¼) config.yamlì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return f"ì˜¤ë¥˜: ëª¨ë¸ '{model_name}' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    provider = model_config.get('provider')
    response = f"ì˜¤ë¥˜: Provider '{provider}'ì´(ê°€) êµ¬í˜„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." 

    try:
        # --- ë”ë¯¸ Provider ---
        if provider == "dummy":
            time.sleep(0.1) 
            if "[ê²€ì¦ ì§ˆë¬¸]" in prompt:
                response = "ë”ë¯¸: ì´ ì‚¬ì‹¤ì€ ì¶œì²˜ì— ë”°ë¼ ì˜¬ë°”ë¥¸ê°€ìš”?"
            elif "[ì‚¬ì‹¤ì  ë‹µë³€]" in prompt:
                response = "ë”ë¯¸: ë„¤, ì¶œì²˜ì—ì„œ ì´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
            elif "[íŒë‹¨]" in prompt:
                response = random.choice(["[ì˜ˆ]", "[ì•„ë‹ˆì˜¤]"])
            elif "[ìˆ˜ì •ëœ íŒ©íŠ¸]" in prompt:
                response = "ë”ë¯¸: ì´ê²ƒì€ ìˆ˜ì •ëœ ì‚¬ì‹¤ì…ë‹ˆë‹¤."
            elif "[ì¬ì‘ì„±ëœ ë¬¸ì¥]" in prompt:
                response = "ë”ë¯¸: ì´ê²ƒì€ ìˆ˜ì •ì„ ë°˜ì˜í•˜ì—¬ ì¬ì‘ì„±ëœ ë¬¸ì¥ì…ë‹ˆë‹¤."
            else:
                response = f"ë”ë¯¸ ì‘ë‹µ: {prompt[:50]}..."
            
            # [ì¶”ê°€] ë”ë¯¸ í† í° ê³„ì‚° (ê·¼ì‚¬ì¹˜: 4ê¸€ì = 1í† í°)
            in_tokens = len(prompt) // 4
            out_tokens = len(response) // 4
            token_tracker.input_tokens += in_tokens
            token_tracker.output_tokens += out_tokens
            token_tracker.total_tokens += (in_tokens + out_tokens)
            
            print(f"ë”ë¯¸ ì‘ë‹µ: {response}")

        # --- ë¡œì»¬ Hugging Face Provider ---
        elif provider == "local_hf":
            model_id = model_config['name']
            cache_key = model_id 

            # ìºì‹œì— ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ë¡œë“œ
            if cache_key not in _loaded_models:
                print(f"ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘: {model_id}...")
                loading_params = model_config.get('loading_params', {})
                quantization_config = None
                quant_type = loading_params.get('quantization')

                if quant_type == "bitsandbytes_4bit":
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.bfloat16 
                    )
                    print("4ë¹„íŠ¸ ì–‘ìí™”(BitsAndBytes) ì‚¬ìš© ì¤‘.")
                elif quant_type == "bitsandbytes_8bit":
                     quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                     print("8ë¹„íŠ¸ ì–‘ìí™”(BitsAndBytes) ì‚¬ìš© ì¤‘.")

                auth_token = None
                if model_config.get('requires_auth_token'):
                    auth_token = _get_huggingface_token(config)
                    if not auth_token:
                        raise ValueError(f"ëª¨ë¸ {model_id}ì€(ëŠ”) ì¸ì¦ í† í°ì´ í•„ìš”í•˜ì§€ë§Œ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    else:
                         print("Hugging Face ì¸ì¦ í† í° ì‚¬ìš© ì¤‘.")

                dtype_str = loading_params.get('torch_dtype', 'auto')
                try:
                    torch_dtype = getattr(torch, dtype_str) if hasattr(torch, dtype_str) else 'auto'
                    print(f"Torch dtype ì„¤ì •: {torch_dtype}")
                except TypeError:
                    print(f"ê²½ê³ : ì˜ëª»ëœ torch_dtype '{dtype_str}'. 'auto'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                    torch_dtype = 'auto'

                model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    quantization_config=quantization_config,
                    device_map=loading_params.get('device_map', 'auto'),
                    torch_dtype=torch_dtype,
                    token=auth_token,
                    trust_remote_code=loading_params.get('trust_remote_code', False),
                )
                tokenizer = AutoTokenizer.from_pretrained(model_id, token=auth_token)
                _loaded_models[cache_key] = {"model": model, "tokenizer": tokenizer}
                print(f"ëª¨ë¸ {model_id} ë¡œë“œ ì™„ë£Œ. Device: {model.device}")
            else:
                pass

            model_data = _loaded_models[cache_key]
            model = model_data["model"]
            tokenizer = model_data["tokenizer"]

            # --- ìƒì„± íŒŒë¼ë¯¸í„° ì¤€ë¹„ ---
            gen_params = config.get('default_generation_params', {}).copy()
            model_gen_params = model_config.get('generation_params', {})
            if model_gen_params:
                gen_params.update(model_gen_params)
            if generation_params_override:
                gen_params.update(generation_params_override)

            # --- ìƒì„± ---
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            outputs = model.generate(
                **inputs,
                temperature=gen_params.get('temperature', 1.0),
                top_p=gen_params.get('top_p', 1.0),
                max_new_tokens=gen_params.get('max_new_tokens', 512),
                repetition_penalty=gen_params.get('repetition_penalty'),
                do_sample=(gen_params.get('temperature', 1.0) > 0.0 and gen_params.get('top_p', 1.0) < 1.0),
                pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id
            )
            
            response_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = tokenizer.decode(response_ids, skip_special_tokens=True)
            
            # [í•µì‹¬ ìˆ˜ì •] ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚° ë° ê¸°ë¡
            input_tokens_count = inputs['input_ids'].shape[1]
            output_tokens_count = len(response_ids)
            
            # ì „ì—­ íŠ¸ë˜ì»¤ ì—…ë°ì´íŠ¸
            token_tracker.input_tokens += input_tokens_count
            token_tracker.output_tokens += output_tokens_count
            token_tracker.total_tokens += (input_tokens_count + output_tokens_count)

            print(f"ë¡œì»¬ HF ì‘ë‹µ: {response}")
            # print(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: Input({input_tokens_count}) + Output({output_tokens_count}) = {input_tokens_count + output_tokens_count}")

        else:
            print(f"*** ì˜¤ë¥˜: Provider '{provider}'ì€(ëŠ”) ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ***")
            response = f"ì˜¤ë¥˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” provider '{provider}'."

    except Exception as e:
        print(f"*** {model_name} ({provider}) ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ***")
        import traceback
        traceback.print_exc()
        response = f"ì˜¤ë¥˜: ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ - {type(e).__name__}"

    return response.strip()

# --- ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ìš© ì˜ˆì‹œ ---
if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ë¡œë“œ (ê²½ë¡œ ì£¼ì˜)
    try:
        from utils import load_config # utils.pyê°€ ê°™ì€ í´ë” ë˜ëŠ” PYTHONPATHì— ìˆì–´ì•¼ í•¨
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, '..', 'config.yaml')
        test_config = load_config(config_path)
    except (ImportError, FileNotFoundError) as e:
        print(f"í…ŒìŠ¤íŠ¸ìš© config ë¡œë“œ ì‹¤íŒ¨ ({e}). í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆ<0xEB><0x9B><0x8D>ë‹ˆë‹¤.")
        test_config = None # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ì§€

    if test_config:
        # ë”ë¯¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\n--- ë”ë¯¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
        dummy_response = generate("í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "dummy-model", test_config)
        print("ë”ë¯¸ í…ŒìŠ¤íŠ¸ ì¶œë ¥:", dummy_response)

        # ë¡œì»¬ HF ëª¨ë¸ í…ŒìŠ¤íŠ¸ (configì— ìˆëŠ” ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½)
        # í…ŒìŠ¤íŠ¸ ì „ í•´ë‹¹ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í™˜ê²½ ì„¤ì •(CUDA ë“±) í•„ìš”
        print("\n--- ë¡œì»¬ HF ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
        try:
            # config.yamlì— ì •ì˜ëœ ë¡œì»¬ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
            local_model_name = "meta-llama/Llama-3.1-8B-Instruct" # ì˜ˆì‹œ, ì‹¤ì œ configì— ìˆëŠ” ì´ë¦„ ì‚¬ìš©
            if any(m['name'] == local_model_name for m in test_config.get('models', [])):
                 # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸, temperature ì•½ê°„ ë‚®ì¶°ì„œ ì¼ê´€ì„± ë³´ê¸°
                 hf_response = generate("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
                                        local_model_name,
                                        test_config,
                                        generation_params_override={"temperature": 0.1, "max_new_tokens": 50})
                 print(f"\n{local_model_name} í…ŒìŠ¤íŠ¸ ì¶œë ¥:", hf_response)
            else:
                 print(f"ê²½ê³ : ëª¨ë¸ '{local_model_name}'ì´(ê°€) config íŒŒì¼ì— ì •ì˜ë˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆ<0xEB><0x9B><0x8D>ë‹ˆë‹¤.")

        except Exception as e:
            print(f"\në¡œì»¬ HF í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("  (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, CUDA ì„¤ì •, ë©”ëª¨ë¦¬ ë¶€ì¡± ë“±ì„ í™•ì¸í•˜ì„¸ìš”)")