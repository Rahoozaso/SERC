import os
from dotenv import load_dotenv
import json
import time
import random
from typing import Dict, Any, Optional
import logging
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from src.utils import token_tracker
from google import genai
from google.genai import types

logger = logging.getLogger(__name__)
load_dotenv()
_loaded_models = {}
_loaded_eval_models = {}
# --- Hugging Face í† í° ë¡œë“œ (í•„ìš”ì‹œ) ---
def _get_huggingface_token(config: Dict[str, Any]) -> Optional[str]:
    """í™˜ê²½ ë³€ìˆ˜ HF_TOKEN ë˜ëŠ” config íŒŒì¼ì—ì„œ Hugging Face í† í°ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    key_name = 'huggingface_token'
    env_var = 'HF_TOKEN' # Hugging Faceê°€ ì‚¬ìš©í•˜ëŠ” í‘œì¤€ í™˜ê²½ ë³€ìˆ˜ ì´ë¦„
    token = os.environ.get(env_var)
    if token:
        print(f"Hugging Face í† í°ì„ í™˜ê²½ ë³€ìˆ˜ {env_var}ì—ì„œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
        return token
    print(f"ê²½ê³ : Hugging Face í† í°ì„ í™˜ê²½ ë³€ìˆ˜ {env_var} ë˜ëŠ” configì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (ì ‘ê·¼ ì œí•œ ëª¨ë¸ ë¡œë”© ë¶ˆê°€)")
    return None


def evaluate_generate(prompt: str, model_name: str, config: Dict[str, Any], 
                      generation_params_override: Optional[Dict[str, Any]] = None) -> str:
    
    # 1. ëª¨ë¸ ì„¤ì •
    model_config = next((m for m in config.get('models', []) if m.get('name') == model_name), None)
    provider = model_config.get('provider') if model_config else ("google" if "gemini" in model_name.lower() else "openai")

    # 2. íŒŒë¼ë¯¸í„°
    gen_params = {"temperature": 0.0}
    if model_config and 'generation_params' in model_config:
        gen_params.update(model_config['generation_params'])
    if generation_params_override:
        gen_params.update(generation_params_override)

    # 3. ì¬ì‹œë„ ë¡œì§
    max_retries = 5
    base_wait = 10

    for attempt in range(max_retries + 1):
        try:
            # ---------------------------------------------------------
            # Provider 1: Google Gemini (New SDK: google-genai)
            # ---------------------------------------------------------
            if provider == "google" or "gemini" in model_name.lower():
                api_key = os.environ.get("GOOGLE_API_KEY")
                if not api_key:
                    raise ValueError("GOOGLE_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                # [ìˆ˜ì •ë¨] configure() ëŒ€ì‹  Client ê°ì²´ ìƒì„±
                client = genai.Client(api_key=api_key)
                
                # ì•ˆì „ ì„¤ì • (ëª¨ë‘ í—ˆìš©)
                safety_settings = [
                    types.SafetySetting(category="HARM_CATEGORY_HATE_SPEECH", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_DANGEROUS_CONTENT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_SEXUALLY_EXPLICIT", threshold="BLOCK_NONE"),
                    types.SafetySetting(category="HARM_CATEGORY_HARASSMENT", threshold="BLOCK_NONE"),
                ]

                # Thinking Config ì„¤ì •
                generate_config = types.GenerateContentConfig(
                    temperature=gen_params.get("temperature", 0.0),
                    max_output_tokens=8192,  # ë„‰ë„‰í•œ í† í°
                    response_mime_type="application/json",
                    
                    # Thinking ì„¤ì • ì ìš©
                    thinking_config=types.ThinkingConfig(
                        include_thoughts=False,
                        thinking_level="LOW"
                    ),
                    safety_settings=safety_settings
                )

                # API í˜¸ì¶œ
                response = client.models.generate_content(
                    model=model_name,
                    contents=prompt,
                    config=generate_config
                )
                
                # ê²°ê³¼ ì²˜ë¦¬
                if response.text:
                    return response.text.strip()
                else:
                    logger.warning(f"[{model_name}] ë¹ˆ ì‘ë‹µ ë°œìƒ.")
                    return json.dumps({"score": 0, "reasoning": "Empty Response", "is_misconception": False})

            # ---------------------------------------------------------
            # Provider 2: OpenAI
            # ---------------------------------------------------------
            elif provider == "openai" or "gpt" in model_name.lower():
                from openai import OpenAI
                client = OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
                api_args = {
                    "model": model_name,
                    "messages": [{"role": "user", "content": prompt}],
                    "temperature": 0.0,
                }
                if gen_params.get("response_mime_type") == "application/json":
                    api_args["response_format"] = {"type": "json_object"}
                
                resp = client.chat.completions.create(**api_args)
                return resp.choices[0].message.content

        except Exception as e:
            error_str = str(e)
            if "429" in error_str or "quota" in error_str.lower() or "RESOURCE_EXHAUSTED" in error_str:
                wait_time = base_wait * (2 ** attempt)
                logger.warning(f"Rate Limit(429) ê°ì§€. {wait_time}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„... ({attempt+1}/{max_retries})")
                time.sleep(wait_time)
            else:
                logger.error(f"API Error: {e}")
                return json.dumps({"score": 0, "reasoning": f"API Error: {str(e)}", "is_misconception": False})

    return json.dumps({"score": 0, "reasoning": "Timeout/RateLimit Failed", "is_misconception": False})


# --- ë©”ì¸ ìƒì„± í•¨ìˆ˜ ---
def generate(prompt: str, model_name: str, config: Dict[str, Any],
             generation_params_override: Optional[Dict[str, Any]] = None) -> str:
    
    print(f"\n--- ëª¨ë¸ í˜¸ì¶œ ì‹œì‘: {model_name} ---")
    print(f"í”„ë¡¬í”„íŠ¸ (ì‹œì‘):\n{prompt[:200]}...\n") 

    # ëª¨ë¸ ì„¤ì • ì°¾ê¸°
    model_config = next((m for m in config.get('models', []) if m.get('name') == model_name), None)
    if not model_config:
        print(f" ì˜¤ë¥˜: ëª¨ë¸ '{model_name}'ì„(ë¥¼) config.yamlì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return f"ì˜¤ë¥˜: ëª¨ë¸ '{model_name}' ì„¤ì •ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    provider = model_config.get('provider')
    response = f"ì˜¤ë¥˜: Provider '{provider}'ì´(ê°€) êµ¬í˜„ë˜ì§€ ì•Šì•˜ê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤." 

    try:
        # --- ë”ë¯¸ Provider ---
        if provider == "dummy":
            time.sleep(0.1) 
            if "[ê²€ì¦ ì§ˆë¬¸]" in prompt:
                response = "ë”ë¯¸: ì´ ì‚¬ì‹¤ì€ ì¶œì²˜ì— ë”°ë¼ ì˜¬ë°”ë¥¸ê°€ìš”?"
            elif "[ì‚¬ì‹¤ì  ë‹µë³€]" in prompt:
                response = "ë”ë¯¸: ë„¤, ì¶œì²˜ì—ì„œ ì´ë¥¼ í™•ì¸í•©ë‹ˆë‹¤."
            elif "[íŒë‹¨]" in prompt:
                response = random.choice(["[ì˜ˆ]", "[ì•„ë‹ˆì˜¤]"])
            elif "[ìˆ˜ì •ëœ íŒ©íŠ¸]" in prompt:
                response = "ë”ë¯¸: ì´ê²ƒì€ ìˆ˜ì •ëœ ì‚¬ì‹¤ì…ë‹ˆë‹¤."
            elif "[ì¬ì‘ì„±ëœ ë¬¸ì¥]" in prompt:
                response = "ë”ë¯¸: ì´ê²ƒì€ ìˆ˜ì •ì„ ë°˜ì˜í•˜ì—¬ ì¬ì‘ì„±ëœ ë¬¸ì¥ì…ë‹ˆë‹¤."
            else:
                response = f"ë”ë¯¸ ì‘ë‹µ: {prompt[:50]}..."
            
            # [ì¶”ê°€] ë”ë¯¸ í† í° ê³„ì‚° (ê·¼ì‚¬ì¹˜: 4ê¸€ì = 1í† í°)
            in_tokens = len(prompt) // 4
            out_tokens = len(response) // 4
            token_tracker.input_tokens += in_tokens
            token_tracker.output_tokens += out_tokens
            token_tracker.total_tokens += (in_tokens + out_tokens)
            
            print(f"ë”ë¯¸ ì‘ë‹µ: {response}")

        # --- ë¡œì»¬ Hugging Face Provider ---
        elif provider == "local_hf":
            model_id = model_config['name']
            cache_key = model_id 

            # ìºì‹œì— ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ê°€ ì—†ìœ¼ë©´ ë¡œë“œ
            if cache_key not in _loaded_models:
                print(f"ë¡œì»¬ ëª¨ë¸ ë¡œë”© ì¤‘: {model_id}...")
                loading_params = model_config.get('loading_params', {})
                quantization_config = None
                quant_type = loading_params.get('quantization')

                if quant_type == "bitsandbytes_4bit":
                    quantization_config = BitsAndBytesConfig(
                        load_in_4bit=True,
                        bnb_4bit_compute_dtype=torch.bfloat16 
                    )
                    print("4ë¹„íŠ¸ ì–‘ìí™”(BitsAndBytes) ì‚¬ìš© ì¤‘.")
                elif quant_type == "bitsandbytes_8bit":
                     quantization_config = BitsAndBytesConfig(load_in_8bit=True)
                     print("8ë¹„íŠ¸ ì–‘ìí™”(BitsAndBytes) ì‚¬ìš© ì¤‘.")

                auth_token = None
                if model_config.get('requires_auth_token'):
                    auth_token = _get_huggingface_token(config)
                    if not auth_token:
                        raise ValueError(f"ëª¨ë¸ {model_id}ì€(ëŠ”) ì¸ì¦ í† í°ì´ í•„ìš”í•˜ì§€ë§Œ ì œê³µë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    else:
                         print("Hugging Face ì¸ì¦ í† í° ì‚¬ìš© ì¤‘.")

                dtype_str = loading_params.get('torch_dtype', 'auto')
                try:
                    torch_dtype = getattr(torch, dtype_str) if hasattr(torch, dtype_str) else 'auto'
                    print(f"Torch dtype ì„¤ì •: {torch_dtype}")
                except TypeError:
                    print(f"ê²½ê³ : ì˜ëª»ëœ torch_dtype '{dtype_str}'. 'auto'ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
                    torch_dtype = 'auto'

                model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    quantization_config=quantization_config,
                    device_map=loading_params.get('device_map', 'auto'),
                    torch_dtype=torch_dtype,
                    token=auth_token,
                    trust_remote_code=loading_params.get('trust_remote_code', False),
                )
                tokenizer = AutoTokenizer.from_pretrained(model_id, token=auth_token)
                _loaded_models[cache_key] = {"model": model, "tokenizer": tokenizer}
                print(f"ëª¨ë¸ {model_id} ë¡œë“œ ì™„ë£Œ. Device: {model.device}")
            else:
                pass

            model_data = _loaded_models[cache_key]
            model = model_data["model"]
            tokenizer = model_data["tokenizer"]

            # --- ìƒì„± íŒŒë¼ë¯¸í„° ì¤€ë¹„ ---
            gen_params = config.get('default_generation_params', {}).copy()
            model_gen_params = model_config.get('generation_params', {})
            if model_gen_params:
                gen_params.update(model_gen_params)
            if generation_params_override:
                gen_params.update(generation_params_override)

            # --- ìƒì„± ---
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
            
            outputs = model.generate(
                **inputs,
                temperature=gen_params.get('temperature', 1.0),
                top_p=gen_params.get('top_p', 1.0),
                max_new_tokens=gen_params.get('max_new_tokens', 512),
                repetition_penalty=gen_params.get('repetition_penalty'),
                do_sample=(gen_params.get('temperature', 1.0) > 0.0 and gen_params.get('top_p', 1.0) < 1.0),
                pad_token_id=tokenizer.eos_token_id or tokenizer.pad_token_id
            )
            
            response_ids = outputs[0][inputs['input_ids'].shape[1]:]
            response = tokenizer.decode(response_ids, skip_special_tokens=True)
            
            # [í•µì‹¬ ìˆ˜ì •] ì •í™•í•œ í† í° ìˆ˜ ê³„ì‚° ë° ê¸°ë¡
            input_tokens_count = inputs['input_ids'].shape[1]
            output_tokens_count = len(response_ids)
            
            # ì „ì—­ íŠ¸ë˜ì»¤ ì—…ë°ì´íŠ¸
            token_tracker.input_tokens += input_tokens_count
            token_tracker.output_tokens += output_tokens_count
            token_tracker.total_tokens += (input_tokens_count + output_tokens_count)

            print(f"ë¡œì»¬ HF ì‘ë‹µ: {response}")
            print(f"ğŸ“Š í† í° ì‚¬ìš©ëŸ‰: Input({input_tokens_count}) + Output({output_tokens_count}) = {input_tokens_count + output_tokens_count}")

        else:
            print(f"*** ì˜¤ë¥˜: Provider '{provider}'ì€(ëŠ”) ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ***")
            response = f"ì˜¤ë¥˜: ì§€ì›ë˜ì§€ ì•ŠëŠ” provider '{provider}'."

    except Exception as e:
        print(f"*** {model_name} ({provider}) ëª¨ë¸ í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e} ***")
        import traceback
        traceback.print_exc()
        response = f"ì˜¤ë¥˜: ìƒì„± ì¤‘ ì˜ˆì™¸ ë°œìƒ - {type(e).__name__}"

    return response.strip()

# --- ì§ì ‘ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ìš© ì˜ˆì‹œ ---
if __name__ == '__main__':
    # í…ŒìŠ¤íŠ¸ìš© ì„¤ì • ë¡œë“œ (ê²½ë¡œ ì£¼ì˜)
    try:
        from utils import load_config # utils.pyê°€ ê°™ì€ í´ë” ë˜ëŠ” PYTHONPATHì— ìˆì–´ì•¼ í•¨
        current_dir = os.path.dirname(os.path.abspath(__file__))
        config_path = os.path.join(current_dir, '..', 'config.yaml')
        test_config = load_config(config_path)
    except (ImportError, FileNotFoundError) as e:
        print(f"í…ŒìŠ¤íŠ¸ìš© config ë¡œë“œ ì‹¤íŒ¨ ({e}). í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆ<0xEB><0x9B><0x8D>ë‹ˆë‹¤.")
        test_config = None # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ë°©ì§€

    if test_config:
        # ë”ë¯¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸
        print("\n--- ë”ë¯¸ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
        dummy_response = generate("í”„ë‘ìŠ¤ì˜ ìˆ˜ë„ëŠ” ë¬´ì—‡ì¸ê°€ìš”?", "dummy-model", test_config)
        print("ë”ë¯¸ í…ŒìŠ¤íŠ¸ ì¶œë ¥:", dummy_response)

        # ë¡œì»¬ HF ëª¨ë¸ í…ŒìŠ¤íŠ¸ (configì— ìˆëŠ” ëª¨ë¸ ì´ë¦„ìœ¼ë¡œ ë³€ê²½)
        # í…ŒìŠ¤íŠ¸ ì „ í•´ë‹¹ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° í™˜ê²½ ì„¤ì •(CUDA ë“±) í•„ìš”
        print("\n--- ë¡œì»¬ HF ëª¨ë¸ í…ŒìŠ¤íŠ¸ ---")
        try:
            # config.yamlì— ì •ì˜ëœ ë¡œì»¬ ëª¨ë¸ ì´ë¦„ ì‚¬ìš©
            local_model_name = "meta-llama/Llama-3.1-8B-Instruct" # ì˜ˆì‹œ, ì‹¤ì œ configì— ìˆëŠ” ì´ë¦„ ì‚¬ìš©
            if any(m['name'] == local_model_name for m in test_config.get('models', [])):
                 # ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸, temperature ì•½ê°„ ë‚®ì¶°ì„œ ì¼ê´€ì„± ë³´ê¸°
                 hf_response = generate("ëŒ€í•œë¯¼êµ­ì˜ ìˆ˜ë„ëŠ” ì–´ë””ì¸ê°€ìš”?",
                                        local_model_name,
                                        test_config,
                                        generation_params_override={"temperature": 0.1, "max_new_tokens": 50})
                 print(f"\n{local_model_name} í…ŒìŠ¤íŠ¸ ì¶œë ¥:", hf_response)
            else:
                 print(f"ê²½ê³ : ëª¨ë¸ '{local_model_name}'ì´(ê°€) config íŒŒì¼ì— ì •ì˜ë˜ì§€ ì•Šì•„ í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆ<0xEB><0x9B><0x8D>ë‹ˆë‹¤.")

        except Exception as e:
            print(f"\në¡œì»¬ HF í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("  (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ, CUDA ì„¤ì •, ë©”ëª¨ë¦¬ ë¶€ì¡± ë“±ì„ í™•ì¸í•˜ì„¸ìš”)")